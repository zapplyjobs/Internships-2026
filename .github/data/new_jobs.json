[
  {
    "job_title": "",
    "employer_name": "AgZen",
    "job_city": "Cambridge",
    "job_state": "MA",
    "job_description": ".fe-681e70c2a649507003c3aab5 { --grid-gutter: calc(var(--sqs-mobile-site-gutter, 6vw) - 11.0px); --cell-max-width: calc( ( var(--sqs-site-max-width, 1500px) - (11.0px * (8 - 1)) ) / 8 ); display: grid; position: relative; grid-area: 1/1/-1/-1; grid-template-rows: repeat(12,minmax(24px, auto)); grid-template-columns: minmax(var(--grid-gutter), 1fr) repeat(8, minmax(0, var(--cell-max-width))) minmax(var(--grid-gutter), 1fr); row-gap: 11.0px; column-gap: 11.0px; overflow-x: hidden; overflow-x: clip; } @media (min-width: 768px) { .background-width--inset .fe-681e70c2a649507003c3aab5 { --inset-padding: calc(var(--sqs-site-gutter) * 2); } .fe-681e70c2a649507003c3aab5 { --grid-gutter: calc(var(--sqs-site-gutter, 4vw) - 11.0px); --cell-max-width: calc( ( var(--sqs-site-max-width, 1500px) - (11.0px * (24 - 1)) ) / 24 ); --inset-padding: 0vw; --row-height-scaling-factor: 0.0215; --container-width: min(var(--sqs-site-max-width, 1500px), calc(100vw - var(--sqs-site-gutter, 4vw) * 2 - var(--inset-padding) )); grid-template-rows: repeat(13,minmax(calc(var(--container-width) * var(--row-height-scaling-factor)), auto)); grid-template-columns: minmax(var(--grid-gutter), 1fr) repeat(24, minmax(0, var(--cell-max-width))) minmax(var(--grid-gutter), 1fr); } } .fe-block-yui_3_17_2_1_1746825311996_2339 { grid-area: 1/2/7/10; z-index: 1; @media (max-width: 767px) { } } .fe-block-yui_3_17_2_1_1746825311996_2339 .sqs-block { justify-content: flex-start; } .fe-block-yui_3_17_2_1_1746825311996_2339 .sqs-block-alignment-wrapper { align-items: flex-start; } @media (min-width: 768px) { .fe-block-yui_3_17_2_1_1746825311996_2339 { grid-area: 1/2/2/26; z-index: 2; position: sticky; top: calc(0px + var(--header-fixed-top-offset, 0px)); } .fe-block-yui_3_17_2_1_1746825311996_2339 .sqs-block { justify-content: flex-start; } .fe-block-yui_3_17_2_1_1746825311996_2339 .sqs-block-alignment-wrapper { align-items: flex-start; } } .fe-block-yui_3_17_2_1_1746825311996_3059 { grid-area: 7/2/13/10; z-index: 2; @media (max-width: 767px) { } } .fe-block-yui_3_17_2_1_1746825311996_3059 .sqs-block { justify-content: flex-start; } .fe-block-yui_3_17_2_1_1746825311996_3059 .sqs-block-alignment-wrapper { align-items: flex-start; } @media (min-width: 768px) { .fe-block-yui_3_17_2_1_1746825311996_3059 { grid-area: 2/2/7/26; z-index: 1; } .fe-block-yui_3_17_2_1_1746825311996_3059 .sqs-block { justify-content: flex-start; } .fe-block-yui_3_17_2_1_1746825311996_3059 .sqs-block-alignment-wrapper { align-items: flex-start; } }",
    "job_apply_link": "https://www.agzen.com/jobs?gh_jid=4048593009",
    "job_posted_at_datetime_utc": "2025-10-21T00:25:19.000Z",
    "job_employment_type": "FULLTIME",
    "job_posted_at": "1h",
    "id": "agzen--cambridge",
    "description_platform": "generic",
    "description_success": true
  },
  {
    "job_title": "Recruiter, Technical",
    "employer_name": "anthropic",
    "job_city": "San Francisco, CA | New York City, NY | Seattle, WA",
    "job_apply_link": "https://job-boards.greenhouse.io/anthropic/jobs/4814348008",
    "job_posted_at_datetime_utc": "2026-01-14T13:17:10-05:00",
    "job_description": "About Anthropic Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.About the role As a Technical Recruiter at Anthropic, you will be instrumental in shaping the future of our organization by identifying, engaging, and hiring the best and brightest minds across a range of disciplines. As we continue to push the boundaries of AI research and development, we need a passionate recruiter who can help us build a world-class team dedicated to creating safe and beneficial AI systems. Responsibilities: Develop and execute strategic recruiting plans to identify and source highly qualified candidates Partner with technical and non-technical hiring managers and interviewers to understand hiring needs, team matching, and required skills and qualifications Implement recruiting processes and programs such as developing targeted outreach campaigns, building connections with industry leaders, and removing any unfair biases from the hiring process Collaborate with leadership and cross-functional partners to understand organizational needs and map out long-term talent acquisition strategies that balance priorities across all technical teams Enhance Anthropic's employer brand to showcase our mission, culture, and values to candidates Provide an exceptional candidate experience throughout the recruiting process You may be a good fit if you: Have 5+ years of experience in recruiting Thrive in fast-paced, dynamic environments and enjoy juggling multiple priorities Demonstrate a passion for AI's potential to positively impact the world and a realistic assessment of its risks and limitations Demonstrate strong technical aptitude and the ability to understand and evaluate technical qualifications Have excellent organizational skills and attention to detail, as well as a proactive mindset and the ability to operate with autonomy Have a proven track record of scaling and building diverse and high-performing teams in a fast-paced, high-growth startup environment Strong candidates may also: Be experimental and open to new, creative recruiting ideas for attracting top technical talent Have experience recruiting for Security, Infrastructure, AI/ML, distributed systems, or other highly technical domains Bring a deep interest in and understanding of AI safety The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (\"OTE\") range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.Annual Salary:$170,000 - $230,000 USDLogistics Education requirements: We require at least a Bachelor's degree in a related field or equivalent experience.Location-based hybrid policy: Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices. Visa sponsorship: We do sponsor visas! However, we aren't able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this. We encourage you to apply even if you do not believe you meet every single qualification. Not all strong candidates will meet every single qualification as listed. Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you're interested in this work. We think AI systems like the ones we're building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.Your safety matters to us. To protect yourself from potential scams, remember that Anthropic recruiters only contact you from @anthropic.com email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you're ever unsure about a communication, don't click any links—visit anthropic.com/careers directly for confirmed position openings. How we're different We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We're an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills. The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI & Compute, Concrete Problems in AI Safety, and Learning from Human Preferences. Come work with us! Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. Guidance on Candidates' AI Usage: Learn about our policy for using AI in our application process",
    "id": "anthropic-recruiter-technical-san-francisco-ca-new-york-city-ny-seattle-wa",
    "source": "greenhouse",
    "source_url": "boards-api.greenhouse.io",
    "source_id": "4814348008",
    "title": "Recruiter, Technical",
    "company_name": "anthropic",
    "company_slug": "anthropic",
    "location": "San Francisco, CA | New York City, NY | Seattle, WA",
    "locations": [
      "San Francisco, CA | New York City, NY | Seattle, WA"
    ],
    "url": "https://job-boards.greenhouse.io/anthropic/jobs/4814348008",
    "departments": [
      "People"
    ],
    "employment_type": null,
    "posted_at": "2026-01-14T13:17:10-05:00",
    "fetched_at": "2026-01-17T04:05:13.932Z",
    "description": "&lt;div class=&quot;content-intro&quot;&gt;&lt;h2&gt;&lt;strong&gt;About Anthropic&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.&lt;/p&gt;&lt;/div&gt;&lt;h2&gt;About the role&lt;/h2&gt;\n&lt;p&gt;As a Technical Recruiter at Anthropic, you will be instrumental in shaping the future of our organization by identifying, engaging, and hiring the best and brightest minds across a range of disciplines. As we continue to push the boundaries of AI research and development, we need a passionate recruiter who can help us build a world-class team dedicated to creating safe and beneficial AI systems.&lt;/p&gt;\n&lt;h2&gt;Responsibilities:&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Develop and execute strategic recruiting plans to identify and source highly qualified candidates&lt;/li&gt;\n&lt;li&gt;Partner with technical and non-technical hiring managers and interviewers to understand hiring needs, team matching, and required skills and qualifications&lt;/li&gt;\n&lt;li&gt;Implement recruiting processes and programs such as developing targeted outreach campaigns, building connections with industry leaders, and removing any unfair biases from the hiring process&lt;/li&gt;\n&lt;li&gt;Collaborate with leadership and cross-functional partners to understand organizational needs and map out long-term talent acquisition strategies that balance priorities across all technical teams&lt;/li&gt;\n&lt;li&gt;Enhance Anthropic&#39;s employer brand to showcase our mission, culture, and values to candidates&lt;/li&gt;\n&lt;li&gt;Provide an exceptional candidate experience throughout the recruiting process&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;You may be a good fit if you:&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Have 5+ years of experience in recruiting&lt;/li&gt;\n&lt;li&gt;Thrive in fast-paced, dynamic environments and enjoy juggling multiple priorities&lt;/li&gt;\n&lt;li&gt;Demonstrate a passion for AI&#39;s potential to positively impact the world and a realistic assessment of its risks and limitations&lt;/li&gt;\n&lt;li&gt;Demonstrate strong technical aptitude and the ability to understand and evaluate technical qualifications&lt;/li&gt;\n&lt;li&gt;Have excellent organizational skills and attention to detail, as well as a proactive mindset and the ability to operate with autonomy&lt;/li&gt;\n&lt;li&gt;Have a proven track record of scaling and building diverse and high-performing teams in a fast-paced, high-growth startup environment&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;Strong candidates may also:&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Be experimental and open to new, creative recruiting ideas for attracting top technical talent&lt;/li&gt;\n&lt;li&gt;Have experience recruiting for Security, Infrastructure, AI/ML, distributed systems, or other highly technical domains&lt;/li&gt;\n&lt;li&gt;Bring a deep interest in and understanding of AI safety&lt;/li&gt;\n&lt;/ul&gt;&lt;div class=&quot;content-pay-transparency&quot;&gt;&lt;div class=&quot;pay-input&quot;&gt;&lt;div class=&quot;description&quot;&gt;&lt;p&gt;The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (&quot;OTE&quot;) range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;title&quot;&gt;Annual Salary:&lt;/div&gt;&lt;div class=&quot;pay-range&quot;&gt;&lt;span&gt;$170,000&lt;/span&gt;&lt;span class=&quot;divider&quot;&gt;&amp;mdash;&lt;/span&gt;&lt;span&gt;$230,000 USD&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;content-conclusion&quot;&gt;&lt;h2&gt;&lt;strong&gt;Logistics&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;strong&gt;Education requirements: &lt;/strong&gt;We require at least a Bachelor&#39;s degree in a related field or equivalent experience.&lt;strong&gt;&lt;br&gt;&lt;br&gt;Location-based hybrid policy:&lt;/strong&gt; Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices.&lt;/p&gt;\n&lt;p&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Visa sponsorship:&lt;/strong&gt;&amp;nbsp;We do sponsor visas! However, we aren&#39;t able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this.&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;We encourage you to apply even if you do not believe you meet every single qualification.&lt;/strong&gt; Not all strong candidates will meet every single qualification as listed.&amp;nbsp; Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you&#39;re interested in this work. We think AI systems like the ones we&#39;re building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.&lt;br&gt;&lt;br&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Your safety matters to us.&lt;/strong&gt;&amp;nbsp;To protect yourself from potential scams, remember that Anthropic recruiters only contact you from&amp;nbsp;@anthropic.com&amp;nbsp;email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you&#39;re ever unsure about a communication, don&#39;t click any links—visit&amp;nbsp;&lt;u data-stringify-type=&quot;underline&quot;&gt;&lt;a class=&quot;c-link c-link--underline&quot; href=&quot;http://anthropic.com/careers&quot; target=&quot;_blank&quot; data-stringify-link=&quot;http://anthropic.com/careers&quot; data-sk=&quot;tooltip_parent&quot; data-remove-tab-index=&quot;true&quot;&gt;anthropic.com/careers&lt;/a&gt;&lt;/u&gt;&amp;nbsp;directly for confirmed position openings.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;How we&#39;re different&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We&#39;re an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills.&lt;/p&gt;\n&lt;p&gt;The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI &amp;amp; Compute, Concrete Problems in AI Safety, and Learning from Human Preferences.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Come work with us!&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. &lt;strong data-stringify-type=&quot;bold&quot;&gt;Guidance on Candidates&#39; AI Usage:&lt;/strong&gt;&amp;nbsp;Learn about&amp;nbsp;&lt;a class=&quot;c-link&quot; href=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; target=&quot;_blank&quot; data-stringify-link=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; data-sk=&quot;tooltip_parent&quot;&gt;our policy&lt;/a&gt;&amp;nbsp;for using AI in our application process&lt;/p&gt;&lt;/div&gt;",
    "_raw": {
      "source": "greenhouse",
      "original_id": 4814348008
    },
    "job_posted_at": "1h",
    "description_platform": "greenhouse",
    "description_success": true
  },
  {
    "job_title": "Recruiting Systems Analyst",
    "employer_name": "anthropic",
    "job_city": "Boston, MA; New York City, NY; San Francisco, CA",
    "job_apply_link": "https://job-boards.greenhouse.io/anthropic/jobs/5065798008",
    "job_posted_at_datetime_utc": "2026-01-14T13:17:10-05:00",
    "job_description": "About Anthropic Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.About the Role We're looking for a Recruiting Systems Analyst to build the integrations and automations that power our recruiting function at scale. You'll work at the intersection of recruiting operations, AI application, and systems engineering—building API integrations, automating workflows across the candidate lifecycle, and ensuring data flows correctly between systems. We're looking for someone who can see a manual process, understand the recruiting context behind it, and ship a working solution. The ideal candidate has experience working within the recruiting or people function and picked up technical skills along the way—or has deep integration experience and genuine curiosity about how talent acquisition actually works. Either way, you understand that the best automation respects the human moments in recruiting while eliminating the tedious ones. What You'll Do Build and maintain integrations between recruiting tools (Greenhouse, Gem, ModernLoop, BrightHire) using APIs, webhooks, and automation platforms like Tines Automate manual workflows across the candidate lifecycle—from sourcing to offer to onboarding handoffs Partner with Recruiting Operations to identify high-impact automation opportunities and translate them into technical solutions Own the full lifecycle from discovery through deployment: scoping requirements with recruiting users, building prototypes, gathering feedback, and iterating to production Troubleshoot and resolve integration issues, ensuring data flows correctly between systems Create documentation and lightweight enablement resources so the team can maintain and extend your work Collaborate with internal engineering departments on shared infrastructure while owning the \"middle layer\" of recruiting-specific automations You May Be a Good Fit If You: Have 5+ years of experience in recruiting operations, talent systems, HR technology, or a similar function—you understand recruiting workflows and why they exist Have hands-on experience building automations and integrations using platforms like Tines, Workato, Zapier, or similar tools Are comfortable working with APIs, webhooks—you can read API documentation and build working integrations independently Have used scripting languages (Python, JavaScript) to solve problems, even if you're not a software engineer by training Excel at translating business problems into technical solutions—you ask the right questions to understand what recruiters actually need Have a track record of building trust with non-technical stakeholders and shipping products they actually use Are willing to do the unglamorous work: sometimes the right solution is a well-built spreadsheet, sometimes it's a complex multi-system integration Are excited about AI's potential to transform knowledge work and care about building systems that are safe and beneficial Strong Candidates May Also Have: Experience with LLMs, prompt engineering, and AI application development Experience administering Greenhouse at a configuration and API level Familiarity with Workday integrations and HRIS data flows Experience with data analysis, reporting tools, or building dashboards Understanding of compliance considerations in recruiting (EEOC, GDPR, etc.) Our Current Tech Stack Greenhouse (ATS) Gem (sourcing & CRM) ModernLoop (scheduling) BrightHire (interview intelligence) Tines (automation) Workday (HRIS) JIRA (ticketing) Outline (documentation) Google Workspace Slack The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (\"OTE\") range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.Annual Salary:$170,000 - $220,000 USDLogistics Education requirements: We require at least a Bachelor's degree in a related field or equivalent experience.Location-based hybrid policy: Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices. Visa sponsorship: We do sponsor visas! However, we aren't able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this. We encourage you to apply even if you do not believe you meet every single qualification. Not all strong candidates will meet every single qualification as listed. Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you're interested in this work. We think AI systems like the ones we're building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.Your safety matters to us. To protect yourself from potential scams, remember that Anthropic recruiters only contact you from @anthropic.com email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you're ever unsure about a communication, don't click any links—visit anthropic.com/careers directly for confirmed position openings. How we're different We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We're an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills. The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI & Compute, Concrete Problems in AI Safety, and Learning from Human Preferences. Come work with us! Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. Guidance on Candidates' AI Usage: Learn about our policy for using AI in our application process",
    "id": "anthropic-recruiting-systems-analyst-boston-ma-new-york-city-ny-san-francisco-ca",
    "source": "greenhouse",
    "source_url": "boards-api.greenhouse.io",
    "source_id": "5065798008",
    "title": "Recruiting Systems Analyst",
    "company_name": "anthropic",
    "company_slug": "anthropic",
    "location": "Boston, MA; New York City, NY; San Francisco, CA",
    "locations": [
      "Boston, MA; New York City, NY; San Francisco, CA"
    ],
    "url": "https://job-boards.greenhouse.io/anthropic/jobs/5065798008",
    "departments": [
      "People"
    ],
    "employment_type": null,
    "posted_at": "2026-01-14T13:17:10-05:00",
    "fetched_at": "2026-01-17T04:05:13.932Z",
    "description": "&lt;div class=&quot;content-intro&quot;&gt;&lt;h2&gt;&lt;strong&gt;About Anthropic&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.&lt;/p&gt;&lt;/div&gt;&lt;h2&gt;&lt;strong&gt;About the Role&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;We&#39;re looking for a Recruiting Systems Analyst to build the integrations and automations that power our recruiting function at scale. You&#39;ll work at the intersection of recruiting operations, AI application, and systems engineering—building API integrations, automating workflows across the candidate lifecycle, and ensuring data flows correctly between systems.&lt;/p&gt;\n&lt;p&gt;We&#39;re looking for someone who can see a manual process, understand the recruiting context behind it, and ship a working solution. The ideal candidate has experience working within the recruiting or people function and picked up technical skills along the way—or has deep integration experience and genuine curiosity about how talent acquisition actually works. Either way, you understand that the best automation respects the human moments in recruiting while eliminating the tedious ones.&lt;/p&gt;\n&lt;h2&gt;What You&#39;ll Do&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;p&gt;Build and maintain integrations between recruiting tools (Greenhouse, Gem,&amp;nbsp;ModernLoop, BrightHire) using APIs, webhooks, and automation platforms like Tines&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Automate manual workflows across the candidate lifecycle—from sourcing to offer to&amp;nbsp;onboarding handoffs&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Partner with Recruiting Operations to identify high-impact automation opportunities and&amp;nbsp;translate them into technical solutions&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Own the full lifecycle from discovery through deployment: scoping requirements with&amp;nbsp;recruiting users, building prototypes, gathering feedback, and iterating to production&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Troubleshoot and resolve integration issues, ensuring data flows correctly between&amp;nbsp;systems&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Create documentation and lightweight enablement resources so the team can maintain&amp;nbsp;and extend your work&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Collaborate with internal engineering departments on shared infrastructure while owning&amp;nbsp;the &quot;middle layer&quot; of recruiting-specific automations&lt;/p&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;&lt;strong&gt;You May Be a Good Fit If You:&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;p&gt;Have 5+ years of experience in recruiting operations, talent systems, HR technology, or&amp;nbsp;a similar function—you understand recruiting workflows and why they exist&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Have hands-on experience building automations and integrations using platforms like&amp;nbsp;Tines, Workato, Zapier, or similar tools&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Are comfortable working with APIs, webhooks—you can read API documentation and&amp;nbsp;build working integrations independently&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Have used scripting languages (Python, JavaScript) to solve problems, even if you&#39;re not&amp;nbsp;a software engineer by training&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Excel at translating business problems into technical solutions—you ask the right&amp;nbsp;questions to understand what recruiters actually need&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Have a track record of building trust with non-technical stakeholders and shipping&amp;nbsp;products they actually use&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Are willing to do the unglamorous work: sometimes the right solution is a well-built spreadsheet, sometimes it&#39;s a complex multi-system integration&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Are excited about AI&#39;s potential to transform knowledge work and care about building&amp;nbsp;systems that are safe and beneficial&lt;/p&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;&lt;strong&gt;Strong Candidates May Also Have:&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;p&gt;Experience with LLMs, prompt engineering, and AI application development&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Experience administering Greenhouse at a configuration and API level&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Familiarity with Workday integrations and HRIS data flows&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Experience with data analysis, reporting tools, or building dashboards&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Understanding of compliance considerations in recruiting (EEOC, GDPR, etc.)&lt;/p&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;p&gt;&lt;strong&gt;Our Current Tech Stack&lt;/strong&gt;&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;p&gt;Greenhouse (ATS)&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Gem (sourcing &amp;amp; CRM)&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;ModernLoop (scheduling)&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;BrightHire (interview intelligence)&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Tines (automation)&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Workday (HRIS)&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;JIRA (ticketing)&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Outline (documentation)&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Google Workspace&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Slack&lt;/p&gt;\n&lt;/li&gt;\n&lt;/ul&gt;&lt;div class=&quot;content-pay-transparency&quot;&gt;&lt;div class=&quot;pay-input&quot;&gt;&lt;div class=&quot;description&quot;&gt;&lt;p&gt;The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (&quot;OTE&quot;) range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;title&quot;&gt;Annual Salary:&lt;/div&gt;&lt;div class=&quot;pay-range&quot;&gt;&lt;span&gt;$170,000&lt;/span&gt;&lt;span class=&quot;divider&quot;&gt;&amp;mdash;&lt;/span&gt;&lt;span&gt;$220,000 USD&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;content-conclusion&quot;&gt;&lt;h2&gt;&lt;strong&gt;Logistics&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;strong&gt;Education requirements: &lt;/strong&gt;We require at least a Bachelor&#39;s degree in a related field or equivalent experience.&lt;strong&gt;&lt;br&gt;&lt;br&gt;Location-based hybrid policy:&lt;/strong&gt; Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices.&lt;/p&gt;\n&lt;p&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Visa sponsorship:&lt;/strong&gt;&amp;nbsp;We do sponsor visas! However, we aren&#39;t able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this.&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;We encourage you to apply even if you do not believe you meet every single qualification.&lt;/strong&gt; Not all strong candidates will meet every single qualification as listed.&amp;nbsp; Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you&#39;re interested in this work. We think AI systems like the ones we&#39;re building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.&lt;br&gt;&lt;br&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Your safety matters to us.&lt;/strong&gt;&amp;nbsp;To protect yourself from potential scams, remember that Anthropic recruiters only contact you from&amp;nbsp;@anthropic.com&amp;nbsp;email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you&#39;re ever unsure about a communication, don&#39;t click any links—visit&amp;nbsp;&lt;u data-stringify-type=&quot;underline&quot;&gt;&lt;a class=&quot;c-link c-link--underline&quot; href=&quot;http://anthropic.com/careers&quot; target=&quot;_blank&quot; data-stringify-link=&quot;http://anthropic.com/careers&quot; data-sk=&quot;tooltip_parent&quot; data-remove-tab-index=&quot;true&quot;&gt;anthropic.com/careers&lt;/a&gt;&lt;/u&gt;&amp;nbsp;directly for confirmed position openings.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;How we&#39;re different&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We&#39;re an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills.&lt;/p&gt;\n&lt;p&gt;The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI &amp;amp; Compute, Concrete Problems in AI Safety, and Learning from Human Preferences.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Come work with us!&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. &lt;strong data-stringify-type=&quot;bold&quot;&gt;Guidance on Candidates&#39; AI Usage:&lt;/strong&gt;&amp;nbsp;Learn about&amp;nbsp;&lt;a class=&quot;c-link&quot; href=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; target=&quot;_blank&quot; data-stringify-link=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; data-sk=&quot;tooltip_parent&quot;&gt;our policy&lt;/a&gt;&amp;nbsp;for using AI in our application process&lt;/p&gt;&lt;/div&gt;",
    "_raw": {
      "source": "greenhouse",
      "original_id": 5065798008
    },
    "job_posted_at": "1h",
    "description_platform": "greenhouse",
    "description_success": true
  },
  {
    "job_title": "Regulatory Counsel",
    "employer_name": "anthropic",
    "job_city": "San Francisco, CA | New York City, NY | Washington, DC",
    "job_apply_link": "https://job-boards.greenhouse.io/anthropic/jobs/4980530008",
    "job_posted_at_datetime_utc": "2026-01-14T13:17:10-05:00",
    "job_description": "About Anthropic Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.About the Role Anthropic is seeking a Regulatory Counsel to help support our global regulatory strategy at Anthropic. In this role, you will navigate the complex and rapidly evolving global regulatory landscape for AI across key markets worldwide. You will be at the forefront of novel legal and compliance challenges in artificial intelligence, advising on the development of scalable compliance programs that support Anthropic's responsible AI development mission. Responsibilities: Analyze, interpret and advise on emerging technology regulations across global markets, including the EU, US, UK, Asia-Pacific, and other key regions Analyze, interpret and advise on the application of pre-existing laws to to reflect changes in Anthropic’s business, operations and products and high-growth trajectory Develop comprehensive compliance strategies for Anthropic that address diverse regulatory frameworks worldwide Lead cross-regional regulatory analysis to identify common requirements and market-specific obligations Support Policy Legal on the identification of potential implementation challenges and risks during the pre-legislative process Partner with Legal, Compliance and business teams to translate global regulatory requirements into actionable compliance frameworks Represent Anthropic in international industry associations, regulatory forums and with other stakeholders Build relationships with key regulators and policymakers across multiple jurisdictions Coordinate with regional legal counsel to ensure consistent regulatory approaches while respecting local requirements Support our Go-To-Market team on the addressing customer and external partner queries in relation to their compliance expectations relating to emerging laws Support the development of Anthropic's global regulatory compliance strategy You may be a good fit if you have: A JD and active membership in at least one U.S. state bar (California preferred) At least 8 years of regulatory legal experience, preferably at hyper-growth technology environments particularly areas such as AI Safety regulation, content moderation/online safety, privacy and ESG Experience navigating complex multi-jurisdictional compliance challenges for technology companies and engaging with regulatory authorities across different cultural and legal contexts A strong understanding of global AI governance developments and policy trends, with aptitude for developing novel compliance approaches in areas without established precedent Excellence in cross-functional collaboration and ability to effectively communicate complex legal concepts to technical, policy, and business teams Passion for responsible AI development while balancing innovation with appropriate safeguards Ability to travel internationally as needed to support global regulatory initiatives Strong candidates may have: A track record of developing novel compliance approaches for cutting-edge technologies with limited regulatory precedent Experience translating technical AI concepts and capabilities into legal and regulatory contexts for diverse stakeholders Background in related regulatory areas that intersect with AI governance (e.g., cybersecurity, critical infrastructure, competition law) Role-specific policy: For this role, we expect all staff to be able to work from our San Francisco, New York or Washington, D.C. office at least 3 days a week, though we encourage you to apply even if you might need some flexibility for an interim period of time.The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (\"OTE\") range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.Annual Salary:$265,000 - $320,000 USDLogistics Education requirements: We require at least a Bachelor's degree in a related field or equivalent experience.Location-based hybrid policy: Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices. Visa sponsorship: We do sponsor visas! However, we aren't able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this. We encourage you to apply even if you do not believe you meet every single qualification. Not all strong candidates will meet every single qualification as listed. Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you're interested in this work. We think AI systems like the ones we're building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.Your safety matters to us. To protect yourself from potential scams, remember that Anthropic recruiters only contact you from @anthropic.com email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you're ever unsure about a communication, don't click any links—visit anthropic.com/careers directly for confirmed position openings. How we're different We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We're an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills. The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI & Compute, Concrete Problems in AI Safety, and Learning from Human Preferences. Come work with us! Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. Guidance on Candidates' AI Usage: Learn about our policy for using AI in our application process",
    "id": "anthropic-regulatory-counsel-san-francisco-ca-new-york-city-ny-washington-dc",
    "source": "greenhouse",
    "source_url": "boards-api.greenhouse.io",
    "source_id": "4980530008",
    "title": "Regulatory Counsel",
    "company_name": "anthropic",
    "company_slug": "anthropic",
    "location": "San Francisco, CA | New York City, NY | Washington, DC",
    "locations": [
      "San Francisco, CA | New York City, NY | Washington, DC"
    ],
    "url": "https://job-boards.greenhouse.io/anthropic/jobs/4980530008",
    "departments": [
      "Legal"
    ],
    "employment_type": null,
    "posted_at": "2026-01-14T13:17:10-05:00",
    "fetched_at": "2026-01-17T04:05:13.932Z",
    "description": "&lt;div class=&quot;content-intro&quot;&gt;&lt;h2&gt;&lt;strong&gt;About Anthropic&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.&lt;/p&gt;&lt;/div&gt;&lt;h2&gt;&lt;strong&gt;About the Role&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic is seeking a Regulatory Counsel to help support our global regulatory strategy at Anthropic. In this role, you will navigate the complex and rapidly evolving global regulatory landscape for AI across key markets worldwide. You will be at the forefront of novel legal and compliance challenges in artificial intelligence, advising on the development of scalable compliance programs that support Anthropic&#39;s responsible AI development mission.&amp;nbsp;&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Responsibilities:&amp;nbsp;&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Analyze, interpret and advise on emerging technology regulations across global markets, including the EU, US, UK, Asia-Pacific, and other key regions&lt;/li&gt;\n&lt;li&gt;Analyze, interpret and advise on the application of pre-existing laws to to reflect changes in Anthropic’s business, operations and products and high-growth trajectory&lt;/li&gt;\n&lt;li&gt;Develop comprehensive compliance strategies for Anthropic that address diverse regulatory frameworks worldwide&lt;/li&gt;\n&lt;li&gt;Lead cross-regional regulatory analysis to identify common requirements and market-specific obligations&lt;/li&gt;\n&lt;li&gt;Support Policy Legal on the identification of potential implementation challenges and risks during the pre-legislative process&lt;/li&gt;\n&lt;li&gt;Partner with Legal, Compliance and business teams to translate global regulatory requirements into actionable compliance frameworks&amp;nbsp;&lt;/li&gt;\n&lt;li&gt;Represent Anthropic in international industry associations, regulatory forums and with other stakeholders&lt;/li&gt;\n&lt;li&gt;Build relationships with key regulators and policymakers across multiple jurisdictions&lt;/li&gt;\n&lt;li&gt;Coordinate with regional legal counsel to ensure consistent regulatory approaches while respecting local requirements&lt;/li&gt;\n&lt;li&gt;Support our Go-To-Market team on the addressing customer and external partner queries in relation to their compliance expectations relating to emerging laws&lt;/li&gt;\n&lt;li&gt;Support the development of Anthropic&#39;s global regulatory compliance strategy&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;&lt;strong&gt;You may be a good fit if you have:&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;A JD and active membership in at least one U.S. state bar (California preferred)&lt;/li&gt;\n&lt;li&gt;At least 8 years of regulatory legal experience, preferably at hyper-growth technology environments particularly areas such as AI Safety regulation, content moderation/online safety, privacy and ESG&lt;/li&gt;\n&lt;li&gt;Experience navigating complex multi-jurisdictional compliance challenges for technology companies and engaging with regulatory authorities across different cultural and legal contexts&lt;/li&gt;\n&lt;li&gt;A strong understanding of global AI governance developments and policy trends, with aptitude for developing novel compliance approaches in areas without established precedent&lt;/li&gt;\n&lt;li&gt;Excellence in cross-functional collaboration and ability to effectively communicate complex legal concepts to technical, policy, and business teams&lt;/li&gt;\n&lt;li&gt;Passion for responsible AI development while balancing innovation with appropriate safeguards&lt;/li&gt;\n&lt;li&gt;Ability to travel internationally as needed to support global regulatory initiatives&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;&lt;strong&gt;Strong candidates may have:&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;A track record of developing novel compliance approaches for cutting-edge technologies with limited regulatory precedent&lt;/li&gt;\n&lt;li&gt;Experience translating technical AI concepts and capabilities into legal and regulatory contexts for diverse stakeholders&lt;/li&gt;\n&lt;li&gt;Background in related regulatory areas that intersect with AI governance (e.g., cybersecurity, critical infrastructure, competition law)&lt;/li&gt;\n&lt;/ul&gt;\n&lt;p&gt;&lt;strong&gt;Role-specific policy:&lt;/strong&gt; For this role, we expect all staff to be able to work from our San Francisco, New York or Washington, D.C. office at least 3 days a week, though we encourage you to apply even if you might need some flexibility for an interim period of time.&lt;/p&gt;&lt;div class=&quot;content-pay-transparency&quot;&gt;&lt;div class=&quot;pay-input&quot;&gt;&lt;div class=&quot;description&quot;&gt;&lt;p&gt;The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (&quot;OTE&quot;) range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;title&quot;&gt;Annual Salary:&lt;/div&gt;&lt;div class=&quot;pay-range&quot;&gt;&lt;span&gt;$265,000&lt;/span&gt;&lt;span class=&quot;divider&quot;&gt;&amp;mdash;&lt;/span&gt;&lt;span&gt;$320,000 USD&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;content-conclusion&quot;&gt;&lt;h2&gt;&lt;strong&gt;Logistics&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;strong&gt;Education requirements: &lt;/strong&gt;We require at least a Bachelor&#39;s degree in a related field or equivalent experience.&lt;strong&gt;&lt;br&gt;&lt;br&gt;Location-based hybrid policy:&lt;/strong&gt; Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices.&lt;/p&gt;\n&lt;p&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Visa sponsorship:&lt;/strong&gt;&amp;nbsp;We do sponsor visas! However, we aren&#39;t able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this.&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;We encourage you to apply even if you do not believe you meet every single qualification.&lt;/strong&gt; Not all strong candidates will meet every single qualification as listed.&amp;nbsp; Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you&#39;re interested in this work. We think AI systems like the ones we&#39;re building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.&lt;br&gt;&lt;br&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Your safety matters to us.&lt;/strong&gt;&amp;nbsp;To protect yourself from potential scams, remember that Anthropic recruiters only contact you from&amp;nbsp;@anthropic.com&amp;nbsp;email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you&#39;re ever unsure about a communication, don&#39;t click any links—visit&amp;nbsp;&lt;u data-stringify-type=&quot;underline&quot;&gt;&lt;a class=&quot;c-link c-link--underline&quot; href=&quot;http://anthropic.com/careers&quot; target=&quot;_blank&quot; data-stringify-link=&quot;http://anthropic.com/careers&quot; data-sk=&quot;tooltip_parent&quot; data-remove-tab-index=&quot;true&quot;&gt;anthropic.com/careers&lt;/a&gt;&lt;/u&gt;&amp;nbsp;directly for confirmed position openings.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;How we&#39;re different&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We&#39;re an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills.&lt;/p&gt;\n&lt;p&gt;The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI &amp;amp; Compute, Concrete Problems in AI Safety, and Learning from Human Preferences.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Come work with us!&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. &lt;strong data-stringify-type=&quot;bold&quot;&gt;Guidance on Candidates&#39; AI Usage:&lt;/strong&gt;&amp;nbsp;Learn about&amp;nbsp;&lt;a class=&quot;c-link&quot; href=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; target=&quot;_blank&quot; data-stringify-link=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; data-sk=&quot;tooltip_parent&quot;&gt;our policy&lt;/a&gt;&amp;nbsp;for using AI in our application process&lt;/p&gt;&lt;/div&gt;",
    "_raw": {
      "source": "greenhouse",
      "original_id": 4980530008
    },
    "job_posted_at": "1h",
    "description_platform": "greenhouse",
    "description_success": true
  },
  {
    "job_title": "Research Engineer – Cybersecurity RL",
    "employer_name": "anthropic",
    "job_city": "San Francisco, CA | New York City, NY",
    "job_apply_link": "https://job-boards.greenhouse.io/anthropic/jobs/5025624008",
    "job_posted_at_datetime_utc": "2026-01-14T13:17:10-05:00",
    "job_description": "About Anthropic Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.About Horizons The Horizons team leads Anthropic's reinforcement learning (RL) research and development, playing a critical role in advancing our AI systems. We've contributed to every Claude release, with significant impact on the autonomy, coding, and reasoning capabilities of Anthropic's models. About the role We're hiring for the Cybersecurity RL team within Horizons. As a Research Engineer, you'll help to safely advance the capabilities of our models in secure coding, vulnerability remediation, and other areas of defensive cybersecurity. This role blends research and engineering, requiring you to both develop novel approaches and realize them in code. Your work will include designing and implementing RL environments, conducting experiments and evaluations, delivering your work into production training runs, and collaborating with other researchers, engineers, and cybersecurity specialists across and outside Anthropic. The role requires domain expertise in cybersecurity paired with interest or experience in training safe AI models. For example, you might be a white hat hacker who's curious about how LLMs could augment or transform your work, a security engineer interested in how AI could help harden systems at scale, or a detection and response professional wondering how models could enhance defensive workflows. You may be a good fit if you: Have experience in cybersecurity research. Have experience with machine learning. Have strong software engineering skills. Can balance research exploration with engineering implementation. Are passionate about AI's potential and committed to developing safe and beneficial systems. Strong candidates may also have: Professional experience in security engineering, fuzzing, detection and response, or other applied defensive work. Experience participating in or building CTF competitions and cyber ranges. Academic research experience in cybersecurity. Familiarity with RL techniques and environments. Familiarity with LLM training methodologies. The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (\"OTE\") range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.Annual Salary:$300,000 - $405,000 USDLogistics Education requirements: We require at least a Bachelor's degree in a related field or equivalent experience.Location-based hybrid policy: Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices. Visa sponsorship: We do sponsor visas! However, we aren't able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this. We encourage you to apply even if you do not believe you meet every single qualification. Not all strong candidates will meet every single qualification as listed. Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you're interested in this work. We think AI systems like the ones we're building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.Your safety matters to us. To protect yourself from potential scams, remember that Anthropic recruiters only contact you from @anthropic.com email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you're ever unsure about a communication, don't click any links—visit anthropic.com/careers directly for confirmed position openings. How we're different We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We're an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills. The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI & Compute, Concrete Problems in AI Safety, and Learning from Human Preferences. Come work with us! Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. Guidance on Candidates' AI Usage: Learn about our policy for using AI in our application process",
    "id": "anthropic-research-engineer-cybersecurity-rl-san-francisco-ca-new-york-city-ny",
    "source": "greenhouse",
    "source_url": "boards-api.greenhouse.io",
    "source_id": "5025624008",
    "title": "Research Engineer – Cybersecurity RL",
    "company_name": "anthropic",
    "company_slug": "anthropic",
    "location": "San Francisco, CA | New York City, NY",
    "locations": [
      "San Francisco, CA | New York City, NY"
    ],
    "url": "https://job-boards.greenhouse.io/anthropic/jobs/5025624008",
    "departments": [
      "AI Research & Engineering"
    ],
    "employment_type": null,
    "posted_at": "2026-01-14T13:17:10-05:00",
    "fetched_at": "2026-01-17T04:05:13.932Z",
    "description": "&lt;div class=&quot;content-intro&quot;&gt;&lt;h2&gt;&lt;strong&gt;About Anthropic&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.&lt;/p&gt;&lt;/div&gt;&lt;h2&gt;&lt;strong&gt;About Horizons&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;The Horizons team leads Anthropic&#39;s reinforcement learning (RL) research and development, playing a critical role in advancing our AI systems. We&#39;ve contributed to every Claude release, with significant impact on the autonomy, coding, and reasoning capabilities of Anthropic&#39;s models.&lt;/p&gt;\n&lt;p&gt;&amp;nbsp;&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;About the role&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;We&#39;re hiring for the Cybersecurity RL team within Horizons. As a Research Engineer, you&#39;ll help to safely advance the capabilities of our models in secure coding, vulnerability remediation, and other areas of defensive cybersecurity.&lt;/p&gt;\n&lt;p&gt;This role blends research and engineering, requiring you to both develop novel approaches and realize them in code. Your work will include designing and implementing RL environments, conducting experiments and evaluations, delivering your work into production training runs, and collaborating with other researchers, engineers, and cybersecurity specialists across and outside Anthropic.&lt;/p&gt;\n&lt;p&gt;The role requires domain expertise in cybersecurity paired with interest or experience in training safe AI models. For example, you might be a white hat hacker who&#39;s curious about how LLMs could augment or transform your work, a security engineer interested in how AI could help harden systems at scale, or a detection and response professional wondering how models could enhance defensive workflows.&lt;/p&gt;\n&lt;p&gt;&amp;nbsp;&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;You may be a good fit if you:&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;p&gt;Have experience in cybersecurity research.&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Have experience with machine learning.&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Have strong software engineering skills.&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Can balance research exploration with engineering implementation.&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Are passionate about AI&#39;s potential and committed to developing safe and beneficial systems.&lt;/p&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;p&gt;&amp;nbsp;&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Strong candidates may also have:&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;p&gt;Professional experience in security engineering, fuzzing, detection and response, or other applied defensive work.&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Experience participating in or building CTF competitions and cyber ranges.&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Academic research experience in cybersecurity.&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Familiarity with RL techniques and environments.&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Familiarity with LLM training methodologies.&lt;/p&gt;\n&lt;/li&gt;\n&lt;/ul&gt;&lt;div class=&quot;content-pay-transparency&quot;&gt;&lt;div class=&quot;pay-input&quot;&gt;&lt;div class=&quot;description&quot;&gt;&lt;p&gt;The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (&quot;OTE&quot;) range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;title&quot;&gt;Annual Salary:&lt;/div&gt;&lt;div class=&quot;pay-range&quot;&gt;&lt;span&gt;$300,000&lt;/span&gt;&lt;span class=&quot;divider&quot;&gt;&amp;mdash;&lt;/span&gt;&lt;span&gt;$405,000 USD&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;content-conclusion&quot;&gt;&lt;h2&gt;&lt;strong&gt;Logistics&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;strong&gt;Education requirements: &lt;/strong&gt;We require at least a Bachelor&#39;s degree in a related field or equivalent experience.&lt;strong&gt;&lt;br&gt;&lt;br&gt;Location-based hybrid policy:&lt;/strong&gt; Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices.&lt;/p&gt;\n&lt;p&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Visa sponsorship:&lt;/strong&gt;&amp;nbsp;We do sponsor visas! However, we aren&#39;t able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this.&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;We encourage you to apply even if you do not believe you meet every single qualification.&lt;/strong&gt; Not all strong candidates will meet every single qualification as listed.&amp;nbsp; Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you&#39;re interested in this work. We think AI systems like the ones we&#39;re building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.&lt;br&gt;&lt;br&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Your safety matters to us.&lt;/strong&gt;&amp;nbsp;To protect yourself from potential scams, remember that Anthropic recruiters only contact you from&amp;nbsp;@anthropic.com&amp;nbsp;email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you&#39;re ever unsure about a communication, don&#39;t click any links—visit&amp;nbsp;&lt;u data-stringify-type=&quot;underline&quot;&gt;&lt;a class=&quot;c-link c-link--underline&quot; href=&quot;http://anthropic.com/careers&quot; target=&quot;_blank&quot; data-stringify-link=&quot;http://anthropic.com/careers&quot; data-sk=&quot;tooltip_parent&quot; data-remove-tab-index=&quot;true&quot;&gt;anthropic.com/careers&lt;/a&gt;&lt;/u&gt;&amp;nbsp;directly for confirmed position openings.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;How we&#39;re different&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We&#39;re an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills.&lt;/p&gt;\n&lt;p&gt;The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI &amp;amp; Compute, Concrete Problems in AI Safety, and Learning from Human Preferences.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Come work with us!&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. &lt;strong data-stringify-type=&quot;bold&quot;&gt;Guidance on Candidates&#39; AI Usage:&lt;/strong&gt;&amp;nbsp;Learn about&amp;nbsp;&lt;a class=&quot;c-link&quot; href=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; target=&quot;_blank&quot; data-stringify-link=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; data-sk=&quot;tooltip_parent&quot;&gt;our policy&lt;/a&gt;&amp;nbsp;for using AI in our application process&lt;/p&gt;&lt;/div&gt;",
    "_raw": {
      "source": "greenhouse",
      "original_id": 5025624008
    },
    "job_posted_at": "1h",
    "description_platform": "greenhouse",
    "description_success": true
  },
  {
    "job_title": "Research Engineer, Interpretability",
    "employer_name": "anthropic",
    "job_city": "San Francisco, CA",
    "job_apply_link": "https://job-boards.greenhouse.io/anthropic/jobs/4980430008",
    "job_posted_at_datetime_utc": "2026-01-14T13:17:10-05:00",
    "job_description": "About Anthropic Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems. About the role: When you see what modern language models are capable of, do you wonder, \"How do these things work? How can we trust them?\" The Interpretability team at Anthropic is working to reverse-engineer how trained models work because we believe that a mechanistic understanding is the most robust way to make advanced systems safe. We’re looking for researchers and engineers to join our efforts. People mean many different things by \"interpretability\". We're focused on mechanistic interpretability, which aims to discover how neural network parameters map to meaningful algorithms. Some useful analogies might be to think of us as trying to do \"biology\" or \"neuroscience\" of neural networks using “microscopes” we build, or as treating neural networks as binary computer programs we're trying to \"reverse engineer\". A few places to learn more about our work and team at a high level are this introduction to Interpretability from our research lead, Chris Olah; a discussion of our work on the Hard Fork podcast produced by the New York Times, and this blog post (and accompanying video) sharing more about some of the engineering challenges we’d had to solve to get these results. Some of our team's notable publications include A Mathematical Framework for Transformer Circuits, In-context Learning and Induction Heads, Toy Models of Superposition, Scaling Monosemanticity, and our Circuits’ Methods and Biology papers. This work builds on ideas from members' work prior to Anthropic such as the original circuits thread, Multimodal Neurons, Activation Atlases, and Building Blocks. We aim to create a solid foundation for mechanistically understanding neural networks and making them safe (see our vision post). In the short term, we have focused on resolving the issue of \"superposition\" (see Toy Models of Superposition, Superposition, Memorization, and Double Descent, and our May 2023 update), which causes the computational units of the models, like neurons and attention heads, to be individually uninterpretable, and on finding ways to decompose models into more interpretable components. Our subsequent work found millions of features in Sonnet, one of our production language models, represents progress in this direction. In our most recent work, we develop methods that allow us to build circuits using features and use this circuits to understand the mechanisms associated with a model's computation and study specific examples of multi-hop reasoning, planning, and chain-of-thought faithfulness on Haiku 3.5, one of our production models.” This is a stepping stone towards our overall goal of mechanistically understanding neural networks. We often collaborate with teams across Anthropic, such as Alignment Science and Societal Impacts to use our work to make Anthropic’s models safer. We also have an Interpretability Architectures project that involves collaborating with Pretraining. Responsibilities: Implement and analyze research experiments, both quickly in toy scenarios and at scale in large models Set up and optimize research workflows to run efficiently and reliably at large scale Build tools and abstractions to support rapid pace of research experimentation Develop and improve tools and infrastructure to support other teams in using Interpretability’s work to improve model safety You may be a good fit if you: Have 5-10+ years of experience building software Are highly proficient in at least one programming language (e.g., Python, Rust, Go, Java) and productive with python Have some experience contributing to empirical AI research projects Have a strong ability to prioritize and direct effort toward the most impactful work and are comfortable operating with ambiguity and questioning assumptions. Prefer fast-moving collaborative projects to extensive solo efforts Want to learn more about machine learning research and its applications and collaborate closely with researchers Care about the societal impacts and ethics of your work Strong candidates may also have experience with: Designing a code base so that anyone can quickly code experiments, launch them, and analyze their results without hitting bugs Optimizing the performance of large-scale distributed systems Collaborating closely with researchers Language modeling with transformers GPUs or Pytorch Representative Projects: Building Garcon, a tool that allows researchers to easily access LLMs internals from a jupyter notebook Setting up and optimizing a pipeline to efficiently collect petabytes of transformer activations and shuffle them. Profiling and optimizing ML training, including parallelizing to many GPUs Make launching ML experiments and manipulating+analyzing the results fast and easy Creating an interactive visualization of attention between tokens in a language model Role Specific Location Policy: This role is based in San Francisco office; however, we are open to considering exceptional candidates for remote work on a case-by-case basis. The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (\"OTE\") range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.Annual Salary:$315,000 - $560,000 USDLogistics Education requirements: We require at least a Bachelor's degree in a related field or equivalent experience.Location-based hybrid policy: Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices. Visa sponsorship: We do sponsor visas! However, we aren't able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this. We encourage you to apply even if you do not believe you meet every single qualification. Not all strong candidates will meet every single qualification as listed. Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you're interested in this work. We think AI systems like the ones we're building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.Your safety matters to us. To protect yourself from potential scams, remember that Anthropic recruiters only contact you from @anthropic.com email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you're ever unsure about a communication, don't click any links—visit anthropic.com/careers directly for confirmed position openings. How we're different We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We're an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills. The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI & Compute, Concrete Problems in AI Safety, and Learning from Human Preferences. Come work with us! Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. Guidance on Candidates' AI Usage: Learn about our policy for using AI in our application process",
    "id": "anthropic-research-engineer-interpretability-san-francisco-ca",
    "source": "greenhouse",
    "source_url": "boards-api.greenhouse.io",
    "source_id": "4980430008",
    "title": "Research Engineer, Interpretability",
    "company_name": "anthropic",
    "company_slug": "anthropic",
    "location": "San Francisco, CA",
    "locations": [
      "San Francisco, CA"
    ],
    "url": "https://job-boards.greenhouse.io/anthropic/jobs/4980430008",
    "departments": [
      "AI Research & Engineering"
    ],
    "employment_type": null,
    "posted_at": "2026-01-14T13:17:10-05:00",
    "fetched_at": "2026-01-17T04:05:13.932Z",
    "description": "&lt;div class=&quot;content-intro&quot;&gt;&lt;h2&gt;&lt;strong&gt;About Anthropic&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.&lt;/p&gt;&lt;/div&gt;&lt;div&gt;\n&lt;h2&gt;About the role:&lt;/h2&gt;\n&lt;/div&gt;\n&lt;div&gt;\n&lt;p&gt;When you see what modern language models are capable of, do you wonder, &quot;How do these things work? How can we trust them?&quot;&lt;/p&gt;\n&lt;p&gt;The Interpretability team at Anthropic is working to reverse-engineer how trained models work because we believe that a mechanistic understanding is the most robust way to make advanced systems safe. We’re looking for researchers and engineers to join our efforts.&amp;nbsp;&lt;/p&gt;\n&lt;p&gt;&amp;nbsp;People mean many different things by &quot;interpretability&quot;. We&#39;re focused on mechanistic interpretability, which aims to discover how neural network parameters map to meaningful algorithms. Some useful analogies might be to think of us as trying to do &quot;biology&quot; or &quot;neuroscience&quot; of neural networks using “microscopes” we build, or as treating neural networks as binary computer programs we&#39;re trying to &quot;reverse engineer&quot;.&lt;/p&gt;\n&lt;p&gt;&amp;nbsp;A few places to learn more about our work and team at a high level are &lt;a href=&quot;https://www.youtube.com/watch?v=TxhhMTOTMDg&quot;&gt;this introduction to Interpretability&lt;/a&gt; from our research lead, &lt;a href=&quot;https://colah.github.io/about.html&quot;&gt;Chris Olah&lt;/a&gt;; a &lt;a href=&quot;https://open.spotify.com/episode/5UF79Uu94ia0fwC32a89LU&quot;&gt;discussion of our work&lt;/a&gt; on the &lt;a href=&quot;https://www.nytimes.com/column/hard-fork&quot;&gt;Hard Fork podcast&lt;/a&gt; produced by the New York Times, and this &lt;a href=&quot;https://www.anthropic.com/research/engineering-challenges-interpretability&quot;&gt;blog post&lt;/a&gt; (and accompanying video) sharing more about some of the engineering challenges we’d had to solve to get these results. Some of our team&#39;s notable publications include &lt;a href=&quot;https://transformer-circuits.pub/2021/framework/index.html&quot;&gt;A Mathematical Framework for Transformer Circuits&lt;/a&gt;, &lt;a href=&quot;https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html&quot;&gt;In-context Learning and Induction Heads&lt;/a&gt;, &lt;a href=&quot;https://transformer-circuits.pub/2022/toy_model/index.html&quot;&gt;Toy Models of Superposition&lt;/a&gt;, &lt;a href=&quot;https://transformer-circuits.pub/2024/scaling-monosemanticity/&quot;&gt;Scaling Monosemanticity&lt;/a&gt;, and our Circuits’ &lt;a href=&quot;https://transformer-circuits.pub/2025/attribution-graphs/methods.html&quot;&gt;Methods&lt;/a&gt; and &lt;a href=&quot;https://transformer-circuits.pub/2025/attribution-graphs/biology.html&quot;&gt;Biology&lt;/a&gt; papers. This work builds on ideas from members&#39; work prior to Anthropic such as the &lt;a href=&quot;https://distill.pub/2020/circuits/&quot;&gt;original circuits thread&lt;/a&gt;, &lt;a href=&quot;https://distill.pub/2021/multimodal-neurons/&quot;&gt;Multimodal Neurons&lt;/a&gt;, &lt;a href=&quot;https://distill.pub/2019/activation-atlas/&quot;&gt;Activation Atlases&lt;/a&gt;, and &lt;a href=&quot;https://distill.pub/2018/building-blocks/&quot;&gt;Building Blocks&lt;/a&gt;.&lt;/p&gt;\n&lt;p&gt;We aim to create a solid foundation for mechanistically understanding neural networks and making them safe (see our &lt;a href=&quot;https://transformer-circuits.pub/2023/interpretability-dreams/index.html&quot;&gt;vision post&lt;/a&gt;). In the short term, we have focused on resolving the issue of &quot;superposition&quot; (see &lt;a href=&quot;https://transformer-circuits.pub/2022/toy_model/index.html&quot;&gt;Toy Models of Superposition&lt;/a&gt;, &lt;a href=&quot;https://transformer-circuits.pub/2023/toy-double-descent/index.html&quot;&gt;Superposition, Memorization, and Double Descent&lt;/a&gt;, and our &lt;a href=&quot;https://transformer-circuits.pub/2023/may-update/index.html&quot;&gt;May 2023 update&lt;/a&gt;), which causes the computational units of the models, like neurons and attention heads, to be individually uninterpretable, and on finding ways to decompose models into more interpretable components. Our subsequent &lt;a href=&quot;https://www.anthropic.com/news/mapping-mind-language-model&quot;&gt;work&lt;/a&gt; found millions of features in Sonnet, one of our production language models, represents progress in this direction. In our most recent work, we develop methods that allow us to build circuits using features and use this circuits to understand the mechanisms associated with a model&#39;s computation and study specific examples of multi-hop reasoning, planning, and chain-of-thought faithfulness on Haiku 3.5, one of our production models.” This is a stepping stone towards our overall goal of mechanistically understanding neural networks.&lt;/p&gt;\n&lt;p&gt;We often collaborate with teams across Anthropic, such as Alignment Science and Societal Impacts to use our work to make Anthropic’s models safer. We also have an &lt;a href=&quot;https://transformer-circuits.pub/2024/april-update/index.html#interpretability-architecture&quot;&gt;Interpretability Architectures project&lt;/a&gt; that involves collaborating with Pretraining.&lt;/p&gt;\n&lt;/div&gt;\n&lt;div&gt;\n&lt;div class=&quot;section page-centered&quot;&gt;\n&lt;h2&gt;Responsibilities:&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Implement and analyze research experiments, both quickly in toy scenarios and at scale in large models&lt;/li&gt;\n&lt;li&gt;Set up and optimize research workflows to run efficiently and reliably at large scale&lt;/li&gt;\n&lt;li&gt;Build tools and abstractions to support rapid pace of research experimentation&lt;/li&gt;\n&lt;li&gt;Develop and improve tools and infrastructure to support other teams in using Interpretability’s work to improve model safety&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;\n&lt;div class=&quot;section page-centered&quot;&gt;\n&lt;h2&gt;You may be a good fit if you:&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Have 5-10+ years of experience building software&lt;/li&gt;\n&lt;li&gt;Are highly proficient in at least one programming language (e.g., Python, Rust, Go, Java) and productive with python&lt;/li&gt;\n&lt;li&gt;Have some experience contributing to empirical AI research projects&lt;/li&gt;\n&lt;li&gt;Have a strong ability to prioritize and direct effort toward the most impactful work and are comfortable operating with ambiguity and questioning assumptions.&lt;/li&gt;\n&lt;li&gt;Prefer fast-moving collaborative projects to extensive solo efforts&lt;/li&gt;\n&lt;li&gt;Want to learn more about machine learning research and its applications and collaborate closely with researchers&lt;/li&gt;\n&lt;li&gt;Care about the societal impacts and ethics of your work&lt;/li&gt;\n&lt;/ul&gt;\n&lt;div class=&quot;section page-centered&quot;&gt;\n&lt;h2&gt;Strong candidates may also have experience with:&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Designing a code base so that anyone can quickly code experiments, launch them, and analyze their results without hitting bugs&lt;/li&gt;\n&lt;li&gt;Optimizing the performance of large-scale distributed systems&lt;/li&gt;\n&lt;li&gt;Collaborating closely with researchers&lt;/li&gt;\n&lt;li&gt;Language modeling with transformers&lt;/li&gt;\n&lt;li&gt;GPUs or Pytorch&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;\n&lt;div class=&quot;section page-centered&quot;&gt;\n&lt;h2&gt;Representative Projects:&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Building &lt;a href=&quot;https://transformer-circuits.pub/2021/garcon/index.html&quot;&gt;Garcon&lt;/a&gt;, a tool that allows researchers to easily access LLMs internals from a jupyter notebook&lt;/li&gt;\n&lt;li&gt;Setting up and optimizing a pipeline to efficiently collect petabytes of transformer activations and shuffle them.&lt;/li&gt;\n&lt;li&gt;Profiling and optimizing ML training, including parallelizing to many GPUs&lt;/li&gt;\n&lt;li&gt;Make launching ML experiments and manipulating+analyzing the results fast and easy&lt;/li&gt;\n&lt;li&gt;Creating an interactive visualization of attention between tokens in a language model&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;&lt;strong&gt;Role Specific Location Policy:&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;This role is based in San Francisco office; however, we are open to considering exceptional candidates for remote work on a case-by-case basis.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;&lt;div class=&quot;content-pay-transparency&quot;&gt;&lt;div class=&quot;pay-input&quot;&gt;&lt;div class=&quot;description&quot;&gt;&lt;p&gt;The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (&quot;OTE&quot;) range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;title&quot;&gt;Annual Salary:&lt;/div&gt;&lt;div class=&quot;pay-range&quot;&gt;&lt;span&gt;$315,000&lt;/span&gt;&lt;span class=&quot;divider&quot;&gt;&amp;mdash;&lt;/span&gt;&lt;span&gt;$560,000 USD&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;content-conclusion&quot;&gt;&lt;h2&gt;&lt;strong&gt;Logistics&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;strong&gt;Education requirements: &lt;/strong&gt;We require at least a Bachelor&#39;s degree in a related field or equivalent experience.&lt;strong&gt;&lt;br&gt;&lt;br&gt;Location-based hybrid policy:&lt;/strong&gt; Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices.&lt;/p&gt;\n&lt;p&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Visa sponsorship:&lt;/strong&gt;&amp;nbsp;We do sponsor visas! However, we aren&#39;t able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this.&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;We encourage you to apply even if you do not believe you meet every single qualification.&lt;/strong&gt; Not all strong candidates will meet every single qualification as listed.&amp;nbsp; Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you&#39;re interested in this work. We think AI systems like the ones we&#39;re building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.&lt;br&gt;&lt;br&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Your safety matters to us.&lt;/strong&gt;&amp;nbsp;To protect yourself from potential scams, remember that Anthropic recruiters only contact you from&amp;nbsp;@anthropic.com&amp;nbsp;email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you&#39;re ever unsure about a communication, don&#39;t click any links—visit&amp;nbsp;&lt;u data-stringify-type=&quot;underline&quot;&gt;&lt;a class=&quot;c-link c-link--underline&quot; href=&quot;http://anthropic.com/careers&quot; target=&quot;_blank&quot; data-stringify-link=&quot;http://anthropic.com/careers&quot; data-sk=&quot;tooltip_parent&quot; data-remove-tab-index=&quot;true&quot;&gt;anthropic.com/careers&lt;/a&gt;&lt;/u&gt;&amp;nbsp;directly for confirmed position openings.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;How we&#39;re different&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We&#39;re an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills.&lt;/p&gt;\n&lt;p&gt;The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI &amp;amp; Compute, Concrete Problems in AI Safety, and Learning from Human Preferences.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Come work with us!&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. &lt;strong data-stringify-type=&quot;bold&quot;&gt;Guidance on Candidates&#39; AI Usage:&lt;/strong&gt;&amp;nbsp;Learn about&amp;nbsp;&lt;a class=&quot;c-link&quot; href=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; target=&quot;_blank&quot; data-stringify-link=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; data-sk=&quot;tooltip_parent&quot;&gt;our policy&lt;/a&gt;&amp;nbsp;for using AI in our application process&lt;/p&gt;&lt;/div&gt;",
    "_raw": {
      "source": "greenhouse",
      "original_id": 4980430008
    },
    "job_posted_at": "1h",
    "description_platform": "greenhouse",
    "description_success": true
  },
  {
    "job_title": "Research Engineer, Model Evaluations",
    "employer_name": "anthropic",
    "job_city": "San Francisco, CA | New York City, NY",
    "job_apply_link": "https://job-boards.greenhouse.io/anthropic/jobs/4990535008",
    "job_posted_at_datetime_utc": "2026-01-14T13:17:10-05:00",
    "job_description": "About Anthropic Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.About the role As a Research Engineer on the Model Evaluations team, you'll lead the design and implementation of Anthropic's evaluation platform—a critical system that shapes how we understand, measure, and improve our models' capabilities and safety. You'll work at the intersection of research and engineering to develop and implement model evaluations that give us insight into emerging capabilities and build robust evaluation infrastructure that directly influences our training decisions and model development roadmap. Your work will be essential to Anthropic's mission of building safe, beneficial AI systems. You'll collaborate closely with training teams, alignment researchers, and safety teams to ensure our models meet the highest standards before deployment. This is a technical leadership role where you'll drive both the strategic vision and hands-on implementation of our evaluation systems. Responsibilities Design novel evaluation methodologies to assess model capabilities across diverse domains including reasoning, safety, helpfulness, and harmlessness Lead the design and architecture of Anthropic's evaluation platform, ensuring it scales with our rapidly evolving model capabilities and research needs Implement and maintain high-throughput evaluation pipelines that run during production training, providing real-time insights to guide training decisions Analyze evaluation results to identify patterns, failure modes, and opportunities for model improvement, translating complex findings into actionable insights Partner with research teams to develop domain-specific evaluations that probe for emerging capabilities and potential risks Build infrastructure to enable rapid iteration on evaluation design, supporting both automated and human-in-the-loop assessment approaches Establish best practices and standards for evaluation development across the organization Mentor team members and contribute to the growth of evaluation expertise at Anthropic Coordinate evaluation efforts during critical training runs, ensuring comprehensive coverage and timely results Contribute to research publications and external communications about evaluation methodologies and findings You may be a good fit if you Have experience designing and implementing evaluation systems for machine learning models, particularly large language models Have demonstrated technical leadership experience, either formally or through leading complex technical projects Are skilled at both systems engineering and experimental design, comfortable building infrastructure while maintaining scientific rigor Have strong programming skills in Python and experience with distributed computing frameworks Can translate between research needs and engineering constraints, finding pragmatic solutions to complex problems Are results-oriented and thrive in fast-paced environments where priorities can shift based on research findings Enjoy collaborative work and can effectively communicate technical concepts to diverse stakeholders Care deeply about AI safety and the societal impacts of the systems we build Have experience with statistical analysis and can draw meaningful conclusions from large-scale experimental data Strong candidates may also have Experience with evaluation during model training, particularly in production environments Familiarity with safety evaluation frameworks and red teaming methodologies Background in psychometrics, experimental psychology, or other fields focused on measurement and assessment Experience with reinforcement learning evaluation or multi-agent systems Contributions to open-source evaluation benchmarks or frameworks Knowledge of prompt engineering and its role in evaluation design Experience managing evaluation infrastructure at scale (thousands of experiments) Published research in machine learning evaluation, benchmarking, or related areas Representative projects Designing comprehensive evaluation suites that assess models across hundreds of capability dimensions Building real-time evaluation dashboards that surface critical insights during multi-week training runs Developing novel evaluation approaches for emerging capabilities like multi-step reasoning or tool use Creating automated systems to detect regression in model performance or safety properties Implementing efficient evaluation sampling strategies that balance coverage with computational constraints Collaborating with external partners to develop industry-standard evaluation benchmarks Building infrastructure to support human evaluation at scale, including quality control and aggregation systems The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (\"OTE\") range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.Annual Salary:$300,000 - $405,000 USDLogistics Education requirements: We require at least a Bachelor's degree in a related field or equivalent experience.Location-based hybrid policy: Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices. Visa sponsorship: We do sponsor visas! However, we aren't able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this. We encourage you to apply even if you do not believe you meet every single qualification. Not all strong candidates will meet every single qualification as listed. Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you're interested in this work. We think AI systems like the ones we're building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.Your safety matters to us. To protect yourself from potential scams, remember that Anthropic recruiters only contact you from @anthropic.com email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you're ever unsure about a communication, don't click any links—visit anthropic.com/careers directly for confirmed position openings. How we're different We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We're an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills. The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI & Compute, Concrete Problems in AI Safety, and Learning from Human Preferences. Come work with us! Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. Guidance on Candidates' AI Usage: Learn about our policy for using AI in our application process",
    "id": "anthropic-research-engineer-model-evaluations-san-francisco-ca-new-york-city-ny",
    "source": "greenhouse",
    "source_url": "boards-api.greenhouse.io",
    "source_id": "4990535008",
    "title": "Research Engineer, Model Evaluations",
    "company_name": "anthropic",
    "company_slug": "anthropic",
    "location": "San Francisco, CA | New York City, NY",
    "locations": [
      "San Francisco, CA | New York City, NY"
    ],
    "url": "https://job-boards.greenhouse.io/anthropic/jobs/4990535008",
    "departments": [
      "AI Research & Engineering"
    ],
    "employment_type": null,
    "posted_at": "2026-01-14T13:17:10-05:00",
    "fetched_at": "2026-01-17T04:05:13.932Z",
    "description": "&lt;div class=&quot;content-intro&quot;&gt;&lt;h2&gt;&lt;strong&gt;About Anthropic&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.&lt;/p&gt;&lt;/div&gt;&lt;h2&gt;&lt;strong&gt;About the role&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;As a Research Engineer on the Model Evaluations team, you&#39;ll lead the design and implementation of Anthropic&#39;s evaluation platform—a critical system that shapes how we understand, measure, and improve our models&#39; capabilities and safety. You&#39;ll work at the intersection of research and engineering to develop and implement model evaluations that give us insight into emerging capabilities and build robust evaluation infrastructure that directly influences our training decisions and model development roadmap.&lt;/p&gt;\n&lt;p&gt;Your work will be essential to Anthropic&#39;s mission of building safe, beneficial AI systems. You&#39;ll collaborate closely with training teams, alignment researchers, and safety teams to ensure our models meet the highest standards before deployment. This is a technical leadership role where you&#39;ll drive both the strategic vision and hands-on implementation of our evaluation systems.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Responsibilities&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Design novel evaluation methodologies to assess model capabilities across diverse domains including reasoning, safety, helpfulness, and harmlessness&lt;/li&gt;\n&lt;li&gt;Lead the design and architecture of Anthropic&#39;s evaluation platform, ensuring it scales with our rapidly evolving model capabilities and research needs&lt;/li&gt;\n&lt;li&gt;Implement and maintain high-throughput evaluation pipelines that run during production training, providing real-time insights to guide training decisions&lt;/li&gt;\n&lt;li&gt;Analyze evaluation results to identify patterns, failure modes, and opportunities for model improvement, translating complex findings into actionable insights&lt;/li&gt;\n&lt;li&gt;Partner with research teams to develop domain-specific evaluations that probe for emerging capabilities and potential risks&lt;/li&gt;\n&lt;li&gt;Build infrastructure to enable rapid iteration on evaluation design, supporting both automated and human-in-the-loop assessment approaches&lt;/li&gt;\n&lt;li&gt;Establish best practices and standards for evaluation development across the organization&lt;/li&gt;\n&lt;li&gt;Mentor team members and contribute to the growth of evaluation expertise at Anthropic&lt;/li&gt;\n&lt;li&gt;Coordinate evaluation efforts during critical training runs, ensuring comprehensive coverage and timely results&lt;/li&gt;\n&lt;li&gt;Contribute to research publications and external communications about evaluation methodologies and findings&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;&lt;strong&gt;You may be a good fit if you&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Have experience designing and implementing evaluation systems for machine learning models, particularly large language models&lt;/li&gt;\n&lt;li&gt;Have demonstrated technical leadership experience, either formally or through leading complex technical projects&lt;/li&gt;\n&lt;li&gt;Are skilled at both systems engineering and experimental design, comfortable building infrastructure while maintaining scientific rigor&lt;/li&gt;\n&lt;li&gt;Have strong programming skills in Python and experience with distributed computing frameworks&lt;/li&gt;\n&lt;li&gt;Can translate between research needs and engineering constraints, finding pragmatic solutions to complex problems&lt;/li&gt;\n&lt;li&gt;Are results-oriented and thrive in fast-paced environments where priorities can shift based on research findings&lt;/li&gt;\n&lt;li&gt;Enjoy collaborative work and can effectively communicate technical concepts to diverse stakeholders&lt;/li&gt;\n&lt;li&gt;Care deeply about AI safety and the societal impacts of the systems we build&lt;/li&gt;\n&lt;li&gt;Have experience with statistical analysis and can draw meaningful conclusions from large-scale experimental data&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;&lt;strong&gt;Strong candidates may also have&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Experience with evaluation during model training, particularly in production environments&lt;/li&gt;\n&lt;li&gt;Familiarity with safety evaluation frameworks and red teaming methodologies&lt;/li&gt;\n&lt;li&gt;Background in psychometrics, experimental psychology, or other fields focused on measurement and assessment&lt;/li&gt;\n&lt;li&gt;Experience with reinforcement learning evaluation or multi-agent systems&lt;/li&gt;\n&lt;li&gt;Contributions to open-source evaluation benchmarks or frameworks&lt;/li&gt;\n&lt;li&gt;Knowledge of prompt engineering and its role in evaluation design&lt;/li&gt;\n&lt;li&gt;Experience managing evaluation infrastructure at scale (thousands of experiments)&lt;/li&gt;\n&lt;li&gt;Published research in machine learning evaluation, benchmarking, or related areas&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;&lt;strong&gt;Representative projects&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Designing comprehensive evaluation suites that assess models across hundreds of capability dimensions&lt;/li&gt;\n&lt;li&gt;Building real-time evaluation dashboards that surface critical insights during multi-week training runs&lt;/li&gt;\n&lt;li&gt;Developing novel evaluation approaches for emerging capabilities like multi-step reasoning or tool use&lt;/li&gt;\n&lt;li&gt;Creating automated systems to detect regression in model performance or safety properties&lt;/li&gt;\n&lt;li&gt;Implementing efficient evaluation sampling strategies that balance coverage with computational constraints&lt;/li&gt;\n&lt;li&gt;Collaborating with external partners to develop industry-standard evaluation benchmarks&lt;/li&gt;\n&lt;li&gt;Building infrastructure to support human evaluation at scale, including quality control and aggregation systems&amp;nbsp;&lt;/li&gt;\n&lt;/ul&gt;&lt;div class=&quot;content-pay-transparency&quot;&gt;&lt;div class=&quot;pay-input&quot;&gt;&lt;div class=&quot;description&quot;&gt;&lt;p&gt;The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (&quot;OTE&quot;) range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;title&quot;&gt;Annual Salary:&lt;/div&gt;&lt;div class=&quot;pay-range&quot;&gt;&lt;span&gt;$300,000&lt;/span&gt;&lt;span class=&quot;divider&quot;&gt;&amp;mdash;&lt;/span&gt;&lt;span&gt;$405,000 USD&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;content-conclusion&quot;&gt;&lt;h2&gt;&lt;strong&gt;Logistics&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;strong&gt;Education requirements: &lt;/strong&gt;We require at least a Bachelor&#39;s degree in a related field or equivalent experience.&lt;strong&gt;&lt;br&gt;&lt;br&gt;Location-based hybrid policy:&lt;/strong&gt; Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices.&lt;/p&gt;\n&lt;p&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Visa sponsorship:&lt;/strong&gt;&amp;nbsp;We do sponsor visas! However, we aren&#39;t able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this.&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;We encourage you to apply even if you do not believe you meet every single qualification.&lt;/strong&gt; Not all strong candidates will meet every single qualification as listed.&amp;nbsp; Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you&#39;re interested in this work. We think AI systems like the ones we&#39;re building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.&lt;br&gt;&lt;br&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Your safety matters to us.&lt;/strong&gt;&amp;nbsp;To protect yourself from potential scams, remember that Anthropic recruiters only contact you from&amp;nbsp;@anthropic.com&amp;nbsp;email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you&#39;re ever unsure about a communication, don&#39;t click any links—visit&amp;nbsp;&lt;u data-stringify-type=&quot;underline&quot;&gt;&lt;a class=&quot;c-link c-link--underline&quot; href=&quot;http://anthropic.com/careers&quot; target=&quot;_blank&quot; data-stringify-link=&quot;http://anthropic.com/careers&quot; data-sk=&quot;tooltip_parent&quot; data-remove-tab-index=&quot;true&quot;&gt;anthropic.com/careers&lt;/a&gt;&lt;/u&gt;&amp;nbsp;directly for confirmed position openings.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;How we&#39;re different&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We&#39;re an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills.&lt;/p&gt;\n&lt;p&gt;The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI &amp;amp; Compute, Concrete Problems in AI Safety, and Learning from Human Preferences.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Come work with us!&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. &lt;strong data-stringify-type=&quot;bold&quot;&gt;Guidance on Candidates&#39; AI Usage:&lt;/strong&gt;&amp;nbsp;Learn about&amp;nbsp;&lt;a class=&quot;c-link&quot; href=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; target=&quot;_blank&quot; data-stringify-link=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; data-sk=&quot;tooltip_parent&quot;&gt;our policy&lt;/a&gt;&amp;nbsp;for using AI in our application process&lt;/p&gt;&lt;/div&gt;",
    "_raw": {
      "source": "greenhouse",
      "original_id": 4990535008
    },
    "job_posted_at": "1h",
    "description_platform": "greenhouse",
    "description_success": true
  },
  {
    "job_title": "Research Engineer, Universes",
    "employer_name": "anthropic",
    "job_city": "Remote-Friendly (Travel-Required) | San Francisco, CA | Seattle, WA | New York City, NY",
    "job_apply_link": "https://job-boards.greenhouse.io/anthropic/jobs/5061517008",
    "job_posted_at_datetime_utc": "2026-01-14T13:17:10-05:00",
    "job_description": "About Anthropic Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.About the Team The Universes team within Research is responsible for training AI models to perform complex, difficult, long-horizon agentic tasks in ultra-realistic settings. We design and implement novel training environments that go far beyond what models can do today — environments where models learn to navigate ambiguity, handle interruptions, maintain context over extended interactions, and exercise judgment in open-ended scenarios. About the Role We're looking for Research Engineers to help us build the next generation of training environments for capable and safe agentic AI. This role blends research and engineering responsibilities, requiring you to both implement novel approaches and contribute to research direction. You'll work on fundamental research in reinforcement learning, designing training environments and methodologies that push the state of the art, and building evaluations that measure genuine capability. Responsibilities: Build the next generation of agentic environments Build rigorous evaluations that measure real capability Collaborate across research and infrastructure teams to ship environments into production training Debug and iterate rapidly across research and production ML stacks Contribute to research culture through technical discussions and collaborative problem-solving You may be a good fit if you: Are highly impact-driven — you care about outcomes, not activity Operate with high agency Have good research taste or senior technical experience, demonstrating good judgment in identifying what actually matters in complex problem spaces Can balance research exploration with engineering implementation Are passionate about the potential impact of AI and are committed to developing safe and beneficial systems Are comfortable with uncertainty and adapt quickly as the landscape shifts Have strong software engineering skills and can build robust infrastructure Enjoy pair programming (we love to pair!) Strong candidates may also have one or more of the following: Have industry experience with large language model training, fine-tuning or evaluation Have industry experience building RL environments, simulation systems, or large-scale ML infrastructure Senior experience in a relevant technical field even if transitioning domains Deep expertise in sandboxing, containerization, VM infrastructure, or distributed systems Published influential work in relevant ML areas The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (\"OTE\") range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.Annual Salary:$500,000 - $850,000 USDLogistics Education requirements: We require at least a Bachelor's degree in a related field or equivalent experience.Location-based hybrid policy: Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices. Visa sponsorship: We do sponsor visas! However, we aren't able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this. We encourage you to apply even if you do not believe you meet every single qualification. Not all strong candidates will meet every single qualification as listed. Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you're interested in this work. We think AI systems like the ones we're building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.Your safety matters to us. To protect yourself from potential scams, remember that Anthropic recruiters only contact you from @anthropic.com email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you're ever unsure about a communication, don't click any links—visit anthropic.com/careers directly for confirmed position openings. How we're different We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We're an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills. The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI & Compute, Concrete Problems in AI Safety, and Learning from Human Preferences. Come work with us! Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. Guidance on Candidates' AI Usage: Learn about our policy for using AI in our application process",
    "id": "anthropic-research-engineer-universes-remote-friendly-travel-required-san-francisco-ca-seattle-wa-new-york-city-ny",
    "source": "greenhouse",
    "source_url": "boards-api.greenhouse.io",
    "source_id": "5061517008",
    "title": "Research Engineer, Universes",
    "company_name": "anthropic",
    "company_slug": "anthropic",
    "location": "Remote-Friendly (Travel-Required) | San Francisco, CA | Seattle, WA | New York City, NY",
    "locations": [
      "Remote-Friendly (Travel-Required) | San Francisco, CA | Seattle, WA | New York City, NY"
    ],
    "url": "https://job-boards.greenhouse.io/anthropic/jobs/5061517008",
    "departments": [
      "AI Research & Engineering"
    ],
    "employment_type": null,
    "posted_at": "2026-01-14T13:17:10-05:00",
    "fetched_at": "2026-01-17T04:05:13.932Z",
    "description": "&lt;div class=&quot;content-intro&quot;&gt;&lt;h2&gt;&lt;strong&gt;About Anthropic&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.&lt;/p&gt;&lt;/div&gt;&lt;h2 class=&quot;heading&quot;&gt;&lt;strong&gt;About the Team&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;The Universes team within Research is responsible for training AI models to perform complex, difficult, long-horizon agentic tasks in ultra-realistic settings. We design and implement novel training environments that go far beyond what models can do today — environments where models learn to navigate ambiguity, handle interruptions, maintain context over extended interactions, and exercise judgment in open-ended scenarios.&lt;/p&gt;\n&lt;h2 class=&quot;heading&quot;&gt;&lt;strong&gt;About the Role&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;We&#39;re looking for Research Engineers to help us build the next generation of training environments for capable and safe agentic AI.&lt;/p&gt;\n&lt;p&gt;This role blends research and engineering responsibilities, requiring you to both implement novel approaches and contribute to research direction. You&#39;ll work on fundamental research in reinforcement learning, designing training environments and methodologies that push the state of the art, and building evaluations that measure genuine capability.&lt;/p&gt;\n&lt;h2 class=&quot;heading&quot;&gt;&lt;strong&gt;Responsibilities:&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;p&gt;Build the next generation of agentic environments&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Build rigorous evaluations that measure real capability&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Collaborate across research and infrastructure teams to ship environments into production training&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Debug and iterate rapidly across research and production ML stacks&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Contribute to research culture through technical discussions and collaborative problem-solving&lt;/p&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2 class=&quot;heading&quot;&gt;&lt;strong&gt;You may be a good fit if you:&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;p&gt;Are highly impact-driven — you care about outcomes, not activity&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Operate with high agency&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Have good research taste or senior technical experience, demonstrating good judgment in identifying what actually matters in complex problem spaces&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Can balance research exploration with engineering implementation&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Are passionate about the potential impact of AI and are committed to developing safe and beneficial systems&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Are comfortable with uncertainty and adapt quickly as the landscape shifts&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Have strong software engineering skills and can build robust infrastructure&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Enjoy pair programming (we love to pair!)&lt;/p&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2 class=&quot;heading&quot;&gt;&lt;strong&gt;Strong candidates may also have one or more of the following:&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;p&gt;Have industry experience with large language model training, fine-tuning or evaluation&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Have industry experience building RL environments, simulation systems, or large-scale ML infrastructure&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Senior experience in a relevant technical field even if transitioning domains&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Deep expertise in sandboxing, containerization, VM infrastructure, or distributed systems&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Published influential work in relevant ML areas&lt;/p&gt;\n&lt;/li&gt;\n&lt;/ul&gt;&lt;div class=&quot;content-pay-transparency&quot;&gt;&lt;div class=&quot;pay-input&quot;&gt;&lt;div class=&quot;description&quot;&gt;&lt;p&gt;The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (&quot;OTE&quot;) range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;title&quot;&gt;Annual Salary:&lt;/div&gt;&lt;div class=&quot;pay-range&quot;&gt;&lt;span&gt;$500,000&lt;/span&gt;&lt;span class=&quot;divider&quot;&gt;&amp;mdash;&lt;/span&gt;&lt;span&gt;$850,000 USD&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;content-conclusion&quot;&gt;&lt;h2&gt;&lt;strong&gt;Logistics&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;strong&gt;Education requirements: &lt;/strong&gt;We require at least a Bachelor&#39;s degree in a related field or equivalent experience.&lt;strong&gt;&lt;br&gt;&lt;br&gt;Location-based hybrid policy:&lt;/strong&gt; Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices.&lt;/p&gt;\n&lt;p&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Visa sponsorship:&lt;/strong&gt;&amp;nbsp;We do sponsor visas! However, we aren&#39;t able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this.&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;We encourage you to apply even if you do not believe you meet every single qualification.&lt;/strong&gt; Not all strong candidates will meet every single qualification as listed.&amp;nbsp; Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you&#39;re interested in this work. We think AI systems like the ones we&#39;re building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.&lt;br&gt;&lt;br&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Your safety matters to us.&lt;/strong&gt;&amp;nbsp;To protect yourself from potential scams, remember that Anthropic recruiters only contact you from&amp;nbsp;@anthropic.com&amp;nbsp;email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you&#39;re ever unsure about a communication, don&#39;t click any links—visit&amp;nbsp;&lt;u data-stringify-type=&quot;underline&quot;&gt;&lt;a class=&quot;c-link c-link--underline&quot; href=&quot;http://anthropic.com/careers&quot; target=&quot;_blank&quot; data-stringify-link=&quot;http://anthropic.com/careers&quot; data-sk=&quot;tooltip_parent&quot; data-remove-tab-index=&quot;true&quot;&gt;anthropic.com/careers&lt;/a&gt;&lt;/u&gt;&amp;nbsp;directly for confirmed position openings.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;How we&#39;re different&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We&#39;re an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills.&lt;/p&gt;\n&lt;p&gt;The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI &amp;amp; Compute, Concrete Problems in AI Safety, and Learning from Human Preferences.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Come work with us!&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. &lt;strong data-stringify-type=&quot;bold&quot;&gt;Guidance on Candidates&#39; AI Usage:&lt;/strong&gt;&amp;nbsp;Learn about&amp;nbsp;&lt;a class=&quot;c-link&quot; href=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; target=&quot;_blank&quot; data-stringify-link=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; data-sk=&quot;tooltip_parent&quot;&gt;our policy&lt;/a&gt;&amp;nbsp;for using AI in our application process&lt;/p&gt;&lt;/div&gt;",
    "_raw": {
      "source": "greenhouse",
      "original_id": 5061517008
    },
    "job_posted_at": "1h",
    "description_platform": "greenhouse",
    "description_success": true
  },
  {
    "job_title": "Research Operations & Strategy Lead, Coding & Cybersecurity Data",
    "employer_name": "anthropic",
    "job_city": "San Francisco, CA | New York City, NY",
    "job_apply_link": "https://job-boards.greenhouse.io/anthropic/jobs/4946306008",
    "job_posted_at_datetime_utc": "2026-01-14T13:17:10-05:00",
    "job_description": "About Anthropic Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.About the Role: As the Research Operations & Strategy Lead for Coding & Cybersecurity Data, you'll build and scale data operations that advance Claude's coding and cybersecurity capabilities. You'll partner with research teams to design and execute data strategies, manage vendor relationships, and own the entire data pipeline from requirements to production. This is a zero-to-one role requiring technical depth to understand what makes high-quality training data, but your focus will be on strategy and execution rather than hands-on engineering. Think technical founder who evolved from writing code to building the business. About the Impact: The data strategies and operations you build will directly determine how well our models can write code, understand software systems, and identify security vulnerabilities. You'll work with world-class researchers on frontier AI capabilities while building the operational infrastructure to scale these efforts. We're looking for someone who gets excited about the challenge of scaling quality - someone who can think strategically about data needs, build the right partnerships, and execute flawlessly. If you thrive at the intersection of technical depth and operational excellence, we'd love to hear from you. Responsibilities: Develop and execute data strategies for coding capabilities, cybersecurity evaluations, and agentic AI research Partner with research leaders to translate technical requirements into operational frameworks Build data collection and evaluation systems through internal tools, vendor partnerships, and new approaches Identify, evaluate, and manage specialized contractors and vendors for technical data collection Implement quality control processes to ensure data meets training requirements Manage multiple complex projects simultaneously, balancing technical needs with delivery timelines Track metrics and communicate progress to stakeholders You may be a good fit if you: Have 3+ years in technical operations, product management, or entrepreneurial experience building from zero to scale Have strong technical foundations - proficiency in Python and understanding of ML workflows and evaluation frameworks Have strong communication skills and can effectively engage with both technical and non-technical stakeholders, both internal and external parties Are familiar with how LLMs work and could describe how models like Claude are trained Are highly organized and can manage multiple parallel workstreams effectively Have a high threshold for navigating ambiguity and can balance setting strategic priorities with rapid, high-quality execution Thrive in fast-paced research environments with shifting priorities and novel technical challenges Are passionate about AI safety and understand the critical importance of high-quality data in building beneficial AI systems Strong candidates may also have: Experience at companies training AI models, agents, or creating AI training data, evaluations, or environments Knowledge of AI safety research methodologies and evaluation frameworks Experience with RLHF or similar human-in-the-loop training methods Domain expertise in software engineering or cybersecurity Track record of building and scaling operations teams The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (\"OTE\") range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.Annual Salary:$250,000 - $365,000 USDLogistics Education requirements: We require at least a Bachelor's degree in a related field or equivalent experience.Location-based hybrid policy: Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices. Visa sponsorship: We do sponsor visas! However, we aren't able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this. We encourage you to apply even if you do not believe you meet every single qualification. Not all strong candidates will meet every single qualification as listed. Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you're interested in this work. We think AI systems like the ones we're building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.Your safety matters to us. To protect yourself from potential scams, remember that Anthropic recruiters only contact you from @anthropic.com email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you're ever unsure about a communication, don't click any links—visit anthropic.com/careers directly for confirmed position openings. How we're different We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We're an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills. The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI & Compute, Concrete Problems in AI Safety, and Learning from Human Preferences. Come work with us! Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. Guidance on Candidates' AI Usage: Learn about our policy for using AI in our application process",
    "id": "anthropic-research-operations-strategy-lead-coding-cybersecurity-data-san-francisco-ca-new-york-city-ny",
    "source": "greenhouse",
    "source_url": "boards-api.greenhouse.io",
    "source_id": "4946306008",
    "title": "Research Operations & Strategy Lead, Coding & Cybersecurity Data",
    "company_name": "anthropic",
    "company_slug": "anthropic",
    "location": "San Francisco, CA | New York City, NY",
    "locations": [
      "San Francisco, CA | New York City, NY"
    ],
    "url": "https://job-boards.greenhouse.io/anthropic/jobs/4946306008",
    "departments": [
      "AI Research & Engineering"
    ],
    "employment_type": null,
    "posted_at": "2026-01-14T13:17:10-05:00",
    "fetched_at": "2026-01-17T04:05:13.932Z",
    "description": "&lt;div class=&quot;content-intro&quot;&gt;&lt;h2&gt;&lt;strong&gt;About Anthropic&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.&lt;/p&gt;&lt;/div&gt;&lt;h2&gt;About the Role:&lt;/h2&gt;\n&lt;p&gt;As the Research Operations &amp;amp; Strategy Lead for Coding &amp;amp; Cybersecurity Data, you&#39;ll build and scale data operations that advance Claude&#39;s coding and cybersecurity capabilities. You&#39;ll partner with research teams to design and execute data strategies, manage vendor relationships, and own the entire data pipeline from requirements to production. This is a zero-to-one role requiring technical depth to understand what makes high-quality training data, but your focus will be on strategy and execution rather than hands-on engineering. Think technical founder who evolved from writing code to building the business.&lt;/p&gt;\n&lt;h2&gt;About the Impact:&amp;nbsp;&lt;/h2&gt;\n&lt;p&gt;The data strategies and operations you build will directly determine how well our models can write code, understand software systems, and identify security vulnerabilities. You&#39;ll work with world-class researchers on frontier AI capabilities while building the operational infrastructure to scale these efforts.&lt;/p&gt;\n&lt;p&gt;We&#39;re looking for someone who gets excited about the challenge of scaling quality - someone who can think strategically about data needs, build the right partnerships, and execute flawlessly. If you thrive at the intersection of technical depth and operational excellence, we&#39;d love to hear from you.&lt;/p&gt;\n&lt;h2&gt;Responsibilities:&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Develop and execute data strategies for coding capabilities, cybersecurity evaluations, and agentic AI research&lt;/li&gt;\n&lt;li&gt;Partner with research leaders to translate technical requirements into operational frameworks&lt;/li&gt;\n&lt;li&gt;Build data collection and evaluation systems through internal tools, vendor partnerships, and new approaches&lt;/li&gt;\n&lt;li&gt;Identify, evaluate, and manage specialized contractors and vendors for technical data collection&lt;/li&gt;\n&lt;li&gt;Implement quality control processes to ensure data meets training requirements&lt;/li&gt;\n&lt;li&gt;Manage multiple complex projects simultaneously, balancing technical needs with delivery timelines&lt;/li&gt;\n&lt;li&gt;Track metrics and communicate progress to stakeholders&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;You may be a good fit if you:&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Have 3+ years in technical operations, product management, or entrepreneurial experience building from zero to scale&lt;/li&gt;\n&lt;li&gt;Have strong technical foundations - proficiency in Python and understanding of ML workflows and evaluation frameworks&lt;/li&gt;\n&lt;li&gt;Have strong communication skills and can effectively engage with both technical and non-technical stakeholders, both internal and external parties&lt;/li&gt;\n&lt;li&gt;Are familiar with how LLMs work and could describe how models like Claude are trained&lt;/li&gt;\n&lt;li&gt;Are highly organized and can manage multiple parallel workstreams effectively&lt;/li&gt;\n&lt;li&gt;Have a high threshold for navigating ambiguity and can balance setting strategic priorities with rapid, high-quality execution&lt;/li&gt;\n&lt;li&gt;Thrive in fast-paced research environments with shifting priorities and novel technical challenges&lt;/li&gt;\n&lt;li&gt;Are passionate about AI safety and understand the critical importance of high-quality data in building beneficial AI systems&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;Strong candidates may also have:&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Experience at companies training AI models, agents, or creating AI training data, evaluations, or environments&lt;/li&gt;\n&lt;li&gt;Knowledge of AI safety research methodologies and evaluation frameworks&lt;/li&gt;\n&lt;li&gt;Experience with RLHF or similar human-in-the-loop training methods&lt;/li&gt;\n&lt;li&gt;Domain expertise in software engineering or cybersecurity&lt;/li&gt;\n&lt;li&gt;Track record of building and scaling operations teams&lt;/li&gt;\n&lt;/ul&gt;&lt;div class=&quot;content-pay-transparency&quot;&gt;&lt;div class=&quot;pay-input&quot;&gt;&lt;div class=&quot;description&quot;&gt;&lt;p&gt;The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (&quot;OTE&quot;) range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;title&quot;&gt;Annual Salary:&lt;/div&gt;&lt;div class=&quot;pay-range&quot;&gt;&lt;span&gt;$250,000&lt;/span&gt;&lt;span class=&quot;divider&quot;&gt;&amp;mdash;&lt;/span&gt;&lt;span&gt;$365,000 USD&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;content-conclusion&quot;&gt;&lt;h2&gt;&lt;strong&gt;Logistics&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;strong&gt;Education requirements: &lt;/strong&gt;We require at least a Bachelor&#39;s degree in a related field or equivalent experience.&lt;strong&gt;&lt;br&gt;&lt;br&gt;Location-based hybrid policy:&lt;/strong&gt; Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices.&lt;/p&gt;\n&lt;p&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Visa sponsorship:&lt;/strong&gt;&amp;nbsp;We do sponsor visas! However, we aren&#39;t able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this.&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;We encourage you to apply even if you do not believe you meet every single qualification.&lt;/strong&gt; Not all strong candidates will meet every single qualification as listed.&amp;nbsp; Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you&#39;re interested in this work. We think AI systems like the ones we&#39;re building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.&lt;br&gt;&lt;br&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Your safety matters to us.&lt;/strong&gt;&amp;nbsp;To protect yourself from potential scams, remember that Anthropic recruiters only contact you from&amp;nbsp;@anthropic.com&amp;nbsp;email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you&#39;re ever unsure about a communication, don&#39;t click any links—visit&amp;nbsp;&lt;u data-stringify-type=&quot;underline&quot;&gt;&lt;a class=&quot;c-link c-link--underline&quot; href=&quot;http://anthropic.com/careers&quot; target=&quot;_blank&quot; data-stringify-link=&quot;http://anthropic.com/careers&quot; data-sk=&quot;tooltip_parent&quot; data-remove-tab-index=&quot;true&quot;&gt;anthropic.com/careers&lt;/a&gt;&lt;/u&gt;&amp;nbsp;directly for confirmed position openings.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;How we&#39;re different&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We&#39;re an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills.&lt;/p&gt;\n&lt;p&gt;The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI &amp;amp; Compute, Concrete Problems in AI Safety, and Learning from Human Preferences.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Come work with us!&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. &lt;strong data-stringify-type=&quot;bold&quot;&gt;Guidance on Candidates&#39; AI Usage:&lt;/strong&gt;&amp;nbsp;Learn about&amp;nbsp;&lt;a class=&quot;c-link&quot; href=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; target=&quot;_blank&quot; data-stringify-link=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; data-sk=&quot;tooltip_parent&quot;&gt;our policy&lt;/a&gt;&amp;nbsp;for using AI in our application process&lt;/p&gt;&lt;/div&gt;",
    "_raw": {
      "source": "greenhouse",
      "original_id": 4946306008
    },
    "job_posted_at": "1h",
    "description_platform": "greenhouse",
    "description_success": true
  },
  {
    "job_title": "Revenue Systems Solution Architect",
    "employer_name": "anthropic",
    "job_city": "San Francisco, CA",
    "job_apply_link": "https://job-boards.greenhouse.io/anthropic/jobs/5068721008",
    "job_posted_at_datetime_utc": "2026-01-14T13:17:10-05:00",
    "job_description": "About Anthropic Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.About the role: We are seeking a Revenue Systems Solution Architect to join our Finance team at Anthropic. In this role, you will own the design, implementation, and optimization of our revenue recognition and billing systems infrastructure. As we continue to scale our consumption-based business model, you will be instrumental in building the systems foundation that enables accurate revenue recognition, streamlined quote-to-cash processes, and efficient month-end close. This is a hands-on role where you'll architect solutions, configure systems, and solve complex technical challenges directly. You'll work at the intersection of Finance, Accounting, and Sales Operations, ensuring our revenue systems accurately reflect the complexities of ASC 606 compliance while supporting Anthropic's growth trajectory. If you thrive in fast-paced environments and enjoy building scalable financial infrastructure from the ground up, come join us in our mission to build safe, transformative AI. Responsibilities: Own the architecture, implementation, and administration of revenue recognition systems (such as Zuora Revenue, Salesforce Revenue Cloud, or similar platforms), ensuring ASC 606 compliance for consumption-based business models Design and maintain CPQ (Configure-Price-Quote) systems architecture, including Salesforce CPQ, Nue.io, or equivalent platforms, ensuring seamless integration with downstream billing and revenue systems Lead month-end close processes related to revenue recognition, identifying and resolving reconciliation issues, automation opportunities, and process bottlenecks Architect integrations between CPQ, billing, revenue recognition, and ERP systems to create a cohesive quote-to-cash technology stack Develop and maintain technical documentation, data flow diagrams, and system configuration guides Partner with Accounting, FP&A, and Sales Operations teams to translate business requirements into scalable technical solutions Troubleshoot complex revenue recognition scenarios, particularly those unique to consumption and usage-based pricing models Drive continuous improvement of revenue systems, identifying opportunities for automation and enhanced accuracy Support audit readiness by maintaining clear system controls, documentation, and data integrity You may be a good fit if you: Have 7+ years of experience implementing and administering revenue recognition systems such as Zuora Revenue, Salesforce Revenue Cloud, RevPro, or similar platforms Have deep expertise in ASC 606 revenue recognition standards, particularly as applied to consumption-based and usage-based business models Have hands-on experience with CPQ platforms (Salesforce CPQ, Nue.io, DealHub, or similar) and their integration with billing and revenue systems Have a track record of solving complex month-end close issues and optimizing revenue accounting processes Are comfortable working directly in systems—configuring, troubleshooting, and building solutions yourself rather than solely managing vendors Possess strong understanding of quote-to-cash processes and the technical architecture that supports them Can communicate complex technical concepts clearly to both technical and non-technical stakeholders Thrive in unstructured environments and are energized by building processes and systems from the ground up Strong candidates may also: Have experience with NetSuite, Stripe, or Metronome integrations Have worked at high-growth technology companies with complex, evolving revenue models Have SQL proficiency for data analysis, reconciliation, and troubleshooting Have experience supporting external audits related to revenue recognition Bring a background in Accounting or Finance, combined with systems expertise Have familiarity with data warehouse integrations for revenue reporting and analytics Deadline to apply: None. Applications will be reviewed on a rolling basis.The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (\"OTE\") range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.Annual Salary:$205,000 - $265,000 USDLogistics Education requirements: We require at least a Bachelor's degree in a related field or equivalent experience.Location-based hybrid policy: Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices. Visa sponsorship: We do sponsor visas! However, we aren't able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this. We encourage you to apply even if you do not believe you meet every single qualification. Not all strong candidates will meet every single qualification as listed. Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you're interested in this work. We think AI systems like the ones we're building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.Your safety matters to us. To protect yourself from potential scams, remember that Anthropic recruiters only contact you from @anthropic.com email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you're ever unsure about a communication, don't click any links—visit anthropic.com/careers directly for confirmed position openings. How we're different We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We're an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills. The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI & Compute, Concrete Problems in AI Safety, and Learning from Human Preferences. Come work with us! Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. Guidance on Candidates' AI Usage: Learn about our policy for using AI in our application process",
    "id": "anthropic-revenue-systems-solution-architect-san-francisco-ca",
    "source": "greenhouse",
    "source_url": "boards-api.greenhouse.io",
    "source_id": "5068721008",
    "title": "Revenue Systems Solution Architect",
    "company_name": "anthropic",
    "company_slug": "anthropic",
    "location": "San Francisco, CA",
    "locations": [
      "San Francisco, CA"
    ],
    "url": "https://job-boards.greenhouse.io/anthropic/jobs/5068721008",
    "departments": [
      "Finance"
    ],
    "employment_type": null,
    "posted_at": "2026-01-14T13:17:10-05:00",
    "fetched_at": "2026-01-17T04:05:13.932Z",
    "description": "&lt;div class=&quot;content-intro&quot;&gt;&lt;h2&gt;&lt;strong&gt;About Anthropic&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic’s mission is to create reliable, interpretable, and steerable AI systems. We want AI to be safe and beneficial for our users and for society as a whole. Our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial AI systems.&lt;/p&gt;&lt;/div&gt;&lt;h2&gt;&lt;strong&gt;About the role:&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;We are seeking a Revenue Systems Solution Architect to join our Finance team at Anthropic. In this role, you will own the design, implementation, and optimization of our revenue recognition and billing systems infrastructure. As we continue to scale our consumption-based business model, you will be instrumental in building the systems foundation that enables accurate revenue recognition, streamlined quote-to-cash processes, and efficient month-end close.&lt;/p&gt;\n&lt;p&gt;This is a hands-on role where you&#39;ll architect solutions, configure systems, and solve complex technical challenges directly. You&#39;ll work at the intersection of Finance, Accounting, and Sales Operations, ensuring our revenue systems accurately reflect the complexities of ASC 606 compliance while supporting Anthropic&#39;s growth trajectory. If you thrive in fast-paced environments and enjoy building scalable financial infrastructure from the ground up, come join us in our mission to build safe, transformative AI.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Responsibilities:&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;p&gt;Own the architecture, implementation, and administration of revenue recognition systems (such as Zuora Revenue, Salesforce Revenue Cloud, or similar platforms), ensuring ASC 606 compliance for consumption-based business models&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Design and maintain CPQ (Configure-Price-Quote) systems architecture, including Salesforce CPQ, &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;http://Nue.io&quot; target=&quot;_blank&quot;&gt;Nue.io&lt;/a&gt;, or equivalent platforms, ensuring seamless integration with downstream billing and revenue systems&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Lead month-end close processes related to revenue recognition, identifying and resolving reconciliation issues, automation opportunities, and process bottlenecks&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Architect integrations between CPQ, billing, revenue recognition, and ERP systems to create a cohesive quote-to-cash technology stack&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Develop and maintain technical documentation, data flow diagrams, and system configuration guides&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Partner with Accounting, FP&amp;amp;A, and Sales Operations teams to translate business requirements into scalable technical solutions&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Troubleshoot complex revenue recognition scenarios, particularly those unique to consumption and usage-based pricing models&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Drive continuous improvement of revenue systems, identifying opportunities for automation and enhanced accuracy&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Support audit readiness by maintaining clear system controls, documentation, and data integrity&lt;/p&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;&lt;strong&gt;You may be a good fit if you:&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;p&gt;Have 7+ years of experience implementing and administering revenue recognition systems such as Zuora Revenue, Salesforce Revenue Cloud, RevPro, or similar platforms&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Have deep expertise in ASC 606 revenue recognition standards, particularly as applied to consumption-based and usage-based business models&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Have hands-on experience with CPQ platforms (Salesforce CPQ, &lt;a class=&quot;text-accent-secondary-100 underline&quot; href=&quot;http://Nue.io&quot; target=&quot;_blank&quot;&gt;Nue.io&lt;/a&gt;, DealHub, or similar) and their integration with billing and revenue systems&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Have a track record of solving complex month-end close issues and optimizing revenue accounting processes&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Are comfortable working directly in systems—configuring, troubleshooting, and building solutions yourself rather than solely managing vendors&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Possess strong understanding of quote-to-cash processes and the technical architecture that supports them&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Can communicate complex technical concepts clearly to both technical and non-technical stakeholders&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Thrive in unstructured environments and are energized by building processes and systems from the ground up&lt;/p&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;h2&gt;&lt;strong&gt;Strong candidates may also:&lt;/strong&gt;&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;\n&lt;p&gt;Have experience with NetSuite, Stripe, or Metronome integrations&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Have worked at high-growth technology companies with complex, evolving revenue models&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Have SQL proficiency for data analysis, reconciliation, and troubleshooting&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Have experience supporting external audits related to revenue recognition&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Bring a background in Accounting or Finance, combined with systems expertise&lt;/p&gt;\n&lt;/li&gt;\n&lt;li&gt;\n&lt;p&gt;Have familiarity with data warehouse integrations for revenue reporting and analytics&lt;/p&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;p&gt;&lt;strong&gt;Deadline to apply: &lt;/strong&gt;None. Applications will be reviewed on a rolling basis.&lt;/p&gt;&lt;div class=&quot;content-pay-transparency&quot;&gt;&lt;div class=&quot;pay-input&quot;&gt;&lt;div class=&quot;description&quot;&gt;&lt;p&gt;The annual compensation range for this role is below. For sales roles, the range provided is the role’s On Target Earnings (&quot;OTE&quot;) range, meaning that the range includes both the sales commissions/sales bonuses target and annual base salary for the role. Our total compensation package for full-time employees includes equity and benefits.&lt;/p&gt;&lt;/div&gt;&lt;div class=&quot;title&quot;&gt;Annual Salary:&lt;/div&gt;&lt;div class=&quot;pay-range&quot;&gt;&lt;span&gt;$205,000&lt;/span&gt;&lt;span class=&quot;divider&quot;&gt;&amp;mdash;&lt;/span&gt;&lt;span&gt;$265,000 USD&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;content-conclusion&quot;&gt;&lt;h2&gt;&lt;strong&gt;Logistics&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;&lt;strong&gt;Education requirements: &lt;/strong&gt;We require at least a Bachelor&#39;s degree in a related field or equivalent experience.&lt;strong&gt;&lt;br&gt;&lt;br&gt;Location-based hybrid policy:&lt;/strong&gt; Currently, we expect all staff to be in one of our offices at least 25% of the time. However, some roles may require more time in our offices.&lt;/p&gt;\n&lt;p&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Visa sponsorship:&lt;/strong&gt;&amp;nbsp;We do sponsor visas! However, we aren&#39;t able to successfully sponsor visas for every role and every candidate. But if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this.&lt;/p&gt;\n&lt;p&gt;&lt;strong&gt;We encourage you to apply even if you do not believe you meet every single qualification.&lt;/strong&gt; Not all strong candidates will meet every single qualification as listed.&amp;nbsp; Research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you&#39;re interested in this work. We think AI systems like the ones we&#39;re building have enormous social and ethical implications. We think this makes representation even more important, and we strive to include a range of diverse perspectives on our team.&lt;br&gt;&lt;br&gt;&lt;strong data-stringify-type=&quot;bold&quot;&gt;Your safety matters to us.&lt;/strong&gt;&amp;nbsp;To protect yourself from potential scams, remember that Anthropic recruiters only contact you from&amp;nbsp;@anthropic.com&amp;nbsp;email addresses. Be cautious of emails from other domains. Legitimate Anthropic recruiters will never ask for money, fees, or banking information before your first day. If you&#39;re ever unsure about a communication, don&#39;t click any links—visit&amp;nbsp;&lt;u data-stringify-type=&quot;underline&quot;&gt;&lt;a class=&quot;c-link c-link--underline&quot; href=&quot;http://anthropic.com/careers&quot; target=&quot;_blank&quot; data-stringify-link=&quot;http://anthropic.com/careers&quot; data-sk=&quot;tooltip_parent&quot; data-remove-tab-index=&quot;true&quot;&gt;anthropic.com/careers&lt;/a&gt;&lt;/u&gt;&amp;nbsp;directly for confirmed position openings.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;How we&#39;re different&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;We believe that the highest-impact AI research will be big science. At Anthropic we work as a single cohesive team on just a few large-scale research efforts. And we value impact — advancing our long-term goals of steerable, trustworthy AI — rather than work on smaller and more specific puzzles. We view AI research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. We&#39;re an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. As such, we greatly value communication skills.&lt;/p&gt;\n&lt;p&gt;The easiest way to understand our research directions is to read our recent research. This research continues many of the directions our team worked on prior to Anthropic, including: GPT-3, Circuit-Based Interpretability, Multimodal Neurons, Scaling Laws, AI &amp;amp; Compute, Concrete Problems in AI Safety, and Learning from Human Preferences.&lt;/p&gt;\n&lt;h2&gt;&lt;strong&gt;Come work with us!&lt;/strong&gt;&lt;/h2&gt;\n&lt;p&gt;Anthropic is a public benefit corporation headquartered in San Francisco. We offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. &lt;strong data-stringify-type=&quot;bold&quot;&gt;Guidance on Candidates&#39; AI Usage:&lt;/strong&gt;&amp;nbsp;Learn about&amp;nbsp;&lt;a class=&quot;c-link&quot; href=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; target=&quot;_blank&quot; data-stringify-link=&quot;https://www.anthropic.com/candidate-ai-guidance&quot; data-sk=&quot;tooltip_parent&quot;&gt;our policy&lt;/a&gt;&amp;nbsp;for using AI in our application process&lt;/p&gt;&lt;/div&gt;",
    "_raw": {
      "source": "greenhouse",
      "original_id": 5068721008
    },
    "job_posted_at": "1h",
    "description_platform": "greenhouse",
    "description_success": true
  }
]